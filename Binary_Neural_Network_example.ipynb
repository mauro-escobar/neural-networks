{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary Neural Network - example.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "U29F5O0apduE",
        "XMD7YqAxpZx0",
        "-wWabIMzS80N"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1aQtwaxcVZIJMbBnMVD7kg1csXEqM3HTs",
      "authorship_tag": "ABX9TyO8N0id89bCTFFkxcOyTSuD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mauro-escobar/neural-networks/blob/main/Binary_Neural_Network_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnk0gAXiSwcL"
      },
      "source": [
        "Following [this example](https://github.com/jaygshah/Binary-Neural-Networks/blob/281ebc7f3e712801c55aa77cef52365ecc2d146d/binary_deterministic_stochastic.py#L284).\n",
        "\n",
        "### Imports and useful functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vcowmXuSoU1"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import psutil\n",
        "import time"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Q_h9GFpfsp"
      },
      "source": [
        "#### `hard_sigmoid`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxiGPHLgUDRH"
      },
      "source": [
        "def hard_sigmoid(x):\n",
        "    return np.clip((x+1.)/2.,0,1)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he-miAfguX_w"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "datasets_dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "\n",
        "def one_hot(x, n):\n",
        "    if type(x) == list:\n",
        "        x = np.array(x)\n",
        "    x = x.flatten()\n",
        "    o_h = np.zeros((len(x), n))\n",
        "    o_h[np.arange(len(x)), x] = 1\n",
        "    return o_h\n",
        "\n",
        "'''\n",
        "def train_validation_split(noTrSamples=1000, noTsSamples=100, \\\n",
        "                        digit_range=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \\\n",
        "                        noTrPerClass=100, noTsPerClass=10, trData=[],trLabels=[]):\n",
        "    \n",
        "    tsX = np.zeros((noTsSamples, 28*28))\n",
        "    trX = np.zeros((noTrSamples, 28*28))\n",
        "    tsY = np.zeros(noTsSamples)\n",
        "    trY = np.zeros(noTrSamples)\n",
        "\n",
        "    count = 0\n",
        "    for ll in digit_range:\n",
        "        # Train data\n",
        "        idl = np.where(trLabels == ll)\n",
        "        idl = idl[0][: noTrPerClass]\n",
        "        idx = list(range(count*noTrPerClass, (count+1)*noTrPerClass))\n",
        "        trX[idx, :] = trData[idl, :]\n",
        "        trY[idx] = trLabels[idl]\n",
        "        # Test data\n",
        "        idl = np.where(tsLabels == ll)\n",
        "        idl = idl[0][: noTsPerClass]\n",
        "        idx = list(range(count*noTsPerClass, (count+1)*noTsPerClass))\n",
        "        tsX[idx, :] = tsData[idl, :]\n",
        "        tsY[idx] = tsLabels[idl]\n",
        "        count += 1\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    test_idx = np.random.permutation(tsX.shape[0])\n",
        "    tsX = tsX[test_idx,:]\n",
        "    tsY = tsY[test_idx]\n",
        "\n",
        "    trX = trX.T\n",
        "    tsX = tsX.T\n",
        "    trY = trY.reshape(1, -1)\n",
        "    tsY = tsY.reshape(1, -1)\n",
        "    return trX, trY, tsX, tsY\n",
        "'''\n",
        "\n",
        "\n",
        "def mnist(noTrSamples=1000, noTsSamples=100, \\\n",
        "          digit_range=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \\\n",
        "          noTrPerClass=100, noTsPerClass=10):\n",
        "    assert noTrSamples==noTrPerClass*len(digit_range), 'noTrSamples and noTrPerClass mismatch'\n",
        "    assert noTsSamples==noTsPerClass*len(digit_range), 'noTrSamples and noTrPerClass mismatch'\n",
        "    '''\n",
        "    data_dir = os.path.join(datasets_dir, 'mnist/')\n",
        "    fd = open(os.path.join(data_dir, 'train-images.idx3-ubyte'))\n",
        "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "    trData = loaded[16:].reshape((60000, 28*28)).astype(float)\n",
        "\n",
        "    fd = open(os.path.join(data_dir, 'train-labels.idx1-ubyte'))\n",
        "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "    trLabels = loaded[8:].reshape((60000)).astype(float)\n",
        "\n",
        "    fd = open(os.path.join(data_dir, 't10k-images.idx3-ubyte'))\n",
        "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "    tsData = loaded[16:].reshape((10000, 28*28)).astype(float)\n",
        "\n",
        "    fd = open(os.path.join(data_dir, 't10k-labels.idx1-ubyte'))\n",
        "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "    tsLabels = loaded[8:].reshape((10000)).astype(float)\n",
        "    '''\n",
        "    (trData, trLabels), (tsData, tsLabels) = tf.keras.datasets.mnist.load_data()\n",
        "    trData = trData.reshape((60000, 28*28)).astype(float)\n",
        "    trLabels = trLabels.reshape((60000)).astype(float)\n",
        "    tsData = tsData.reshape((10000, 28*28)).astype(float)\n",
        "    tsLabels = tsLabels.reshape((10000)).astype(float)\n",
        "\n",
        "    trData = trData/255.\n",
        "    tsData = tsData/255.\n",
        "\n",
        "    tsX = np.zeros((noTsSamples, 28*28))\n",
        "    trX = np.zeros((noTrSamples, 28*28))\n",
        "    tsY = np.zeros(noTsSamples)\n",
        "    trY = np.zeros(noTrSamples)\n",
        "\n",
        "    count = 0\n",
        "    for ll in digit_range:\n",
        "        # Train data\n",
        "        idl = np.where(trLabels == ll)\n",
        "        idl = idl[0][: noTrPerClass]\n",
        "        idx = list(range(count*noTrPerClass, (count+1)*noTrPerClass))\n",
        "        trX[idx, :] = trData[idl, :]\n",
        "        trY[idx] = trLabels[idl]\n",
        "        # Test data\n",
        "        idl = np.where(tsLabels == ll)\n",
        "        idl = idl[0][: noTsPerClass]\n",
        "        idx = list(range(count*noTsPerClass, (count+1)*noTsPerClass))\n",
        "        tsX[idx, :] = tsData[idl, :]\n",
        "        tsY[idx] = tsLabels[idl]\n",
        "        count += 1\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    test_idx = np.random.permutation(tsX.shape[0])\n",
        "    tsX = tsX[test_idx,:]\n",
        "    tsY = tsY[test_idx]\n",
        "\n",
        "    trX = trX.T\n",
        "    tsX = tsX.T\n",
        "    trY = trY.reshape(1, -1)\n",
        "    tsY = tsY.reshape(1, -1)\n",
        "    return trX, trY, tsX, tsY"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aab3SCqST03E"
      },
      "source": [
        "### Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U29F5O0apduE"
      },
      "source": [
        "#### `relu` and `relu_der`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr09zpPoTNo0"
      },
      "source": [
        "def relu(Z):\n",
        "    '''\n",
        "    computes relu activation of Z\n",
        "    Inputs: \n",
        "        Z is a numpy.ndarray (n, m)\n",
        "    Returns: \n",
        "        A is activation. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}\n",
        "    '''\n",
        "    A = np.maximum(0,Z)\n",
        "    cache = {}\n",
        "    cache[\"Z\"] = Z\n",
        "    return A, cache"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAdYnjpR3pb_"
      },
      "source": [
        "def relu_der(dA, cache):\n",
        "    '''\n",
        "    computes derivative of relu activation\n",
        "    Inputs: \n",
        "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}, where Z was the input \n",
        "        to the activation layer during forward propagation\n",
        "    Returns: \n",
        "        dZ is the derivative. numpy.ndarray (n,m)\n",
        "    '''\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    Z = cache[\"Z\"]\n",
        "    dZ[Z<0] = 0\n",
        "    return dZ"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMD7YqAxpZx0"
      },
      "source": [
        "#### `linear` and `linear_der`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWxkeyTj35Yr"
      },
      "source": [
        "def linear(Z):\n",
        "    '''\n",
        "    computes linear activation of Z\n",
        "    This function is implemented for completeness\n",
        "    Inputs: \n",
        "        Z is a numpy.ndarray (n, m)\n",
        "    Returns: \n",
        "        A is activation. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}\n",
        "    '''\n",
        "    A = Z\n",
        "    cache = {}\n",
        "    return A, cache"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIJavsSt5NKp"
      },
      "source": [
        "def linear_der(dA, cache):\n",
        "    '''\n",
        "    computes derivative of linear activation\n",
        "    This function is implemented for completeness\n",
        "    Inputs: \n",
        "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}, where Z was the input \n",
        "        to the activation layer during forward propagation\n",
        "    Returns: \n",
        "        dZ is the derivative. numpy.ndarray (n,m)\n",
        "    '''\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    return dZ"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wWabIMzS80N"
      },
      "source": [
        "#### `sigmoid` and `sigmoid_der`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqi2wfdwSDqN"
      },
      "source": [
        "def sigmoid(Z):\n",
        "    '''\n",
        "    computes sigmoid activation of Z\n",
        "    Inputs: \n",
        "        Z is a numpy.ndarray (n, m)\n",
        "    Returns: \n",
        "        A is activation. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}\n",
        "    '''\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = {}\n",
        "    cache[\"Z\"] = Z\n",
        "    return A, cache"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxtSoWg4TGIz"
      },
      "source": [
        "def sigmoid_der(dA, cache):\n",
        "    '''\n",
        "    computes derivative of sigmoid activation\n",
        "    Inputs: \n",
        "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}, where Z was the input \n",
        "        to the activation layer during forward propagation\n",
        "    Returns: \n",
        "        dZ is the derivative. numpy.ndarray (n,m)\n",
        "    '''\n",
        "    Z = cache[\"Z\"]\n",
        "    sig, _ = sigmoid(Z)\n",
        "    dZ = dA * sig * (1-sig)\n",
        "    return dZ"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-eOoP16zMlo"
      },
      "source": [
        "#### `tanh` and `tanh_der`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oBzTYU0x4wl"
      },
      "source": [
        "def tanh(Z):\n",
        "    '''\n",
        "    computes tanh activation of Z\n",
        "    Inputs: \n",
        "        Z is a numpy.ndarray (n, m)\n",
        "    Returns: \n",
        "        A is activation. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}\n",
        "    '''\n",
        "    A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
        "    cache = {}\n",
        "    cache[\"Z\"] = Z\n",
        "    return A, cache"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lMXIzcRzSB5"
      },
      "source": [
        "def tanh_der(dA, cache):\n",
        "    '''\n",
        "    computes derivative of tanh activation\n",
        "    Inputs: \n",
        "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}, where Z was the input \n",
        "        to the activation layer during forward propagation\n",
        "    Returns: \n",
        "        dZ is the derivative. numpy.ndarray (n,m)\n",
        "    '''\n",
        "    Z = cache[\"Z\"]\n",
        "    t, _ = tanh(Z)\n",
        "    dZ = dA * (1-t*t)\n",
        "    return dZ"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLtiN7HQT6ev"
      },
      "source": [
        "### Binarizing functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCGwU5ILpTOB"
      },
      "source": [
        "#### `Binarize_Deterministic`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmseIMfQTYdr"
      },
      "source": [
        "def Binarize_Deterministic(W,b):\n",
        "    '''\n",
        "    Input: activation of Z and Binarizing Weights\n",
        "    Output: Binarized weights and activations\n",
        "    '''\n",
        "    threshold,upper,lower=0,1,-1\n",
        "    \n",
        "    Wb = np.zeros(W.shape)\n",
        "    bb = np.zeros(b.shape)\n",
        "    Wb[W>=threshold] = upper\n",
        "    Wb[W<threshold] = lower\n",
        "    bb[b>=threshold] = upper\n",
        "    bb[b<threshold] = lower\n",
        "    \n",
        "    return Wb,bb"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIeKC_z3pRO5"
      },
      "source": [
        "#### `Binarize_Deterministic_A`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KvH4R9ETo-9"
      },
      "source": [
        "def Binarize_Deterministic_A(A):\n",
        "    '''\n",
        "    Input: activation A \n",
        "    Output: Binarized A\n",
        "    '''\n",
        "    threshold,upper,lower=0,1,-1\n",
        "    \n",
        "    # A=Z\n",
        "    Ab = np.zeros(A.shape)\t\n",
        "    Ab[A>=threshold] = upper\n",
        "    Ab[A<threshold] = lower\n",
        "    \n",
        "    return Ab"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muQjKikBpO_b"
      },
      "source": [
        "#### `Binarize_Stochastic`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzx_CMsSoFfm"
      },
      "source": [
        "def Binarize_Stochastic(W,b):\t\n",
        "    #srng = RandomStreams(lasagne.random.get_rng().randint(1, 21))\n",
        "    Wb = hard_sigmoid(W/1.0)\n",
        "    #print(Wb.shape)\n",
        "    Wb[Wb>=0.5] = 1\n",
        "    Wb[Wb<0.5] = -1\t\n",
        "    bb = hard_sigmoid(b/1.0)\n",
        "      \n",
        "    return Wb,bb"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbAysLUH5axZ"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie-3HGi9pLl-"
      },
      "source": [
        "#### `computeLoss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmBukYvU5gcN"
      },
      "source": [
        "def computeLoss(A,Y):\n",
        "    # Calculating one hot vecs\n",
        "    \n",
        "    Y = Y.T\n",
        "    row,col = Y.shape\n",
        "    \n",
        "    # Computing one_hot representation\n",
        "    o_h = np.zeros((row,10))\n",
        "    for i in range(0,row):\n",
        "        temp = np.zeros((10,))\n",
        "        #print Y[i][0]\n",
        "        temp[int(Y[i][0])] = 1\n",
        "        o_h[i] = temp\n",
        "    \n",
        "    A = np.log(A)\n",
        "    \n",
        "    total_cost = -1*np.multiply(o_h,A)\n",
        "    \n",
        "    avg_loss = np.sum(total_cost)/row\n",
        "    \n",
        "    return avg_loss"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjtwszlCpKS4"
      },
      "source": [
        "#### `softmax_cross_entropy_loss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w7m3Ewq9GCk"
      },
      "source": [
        "def softmax_cross_entropy_loss(Z, Y=np.array([])):\n",
        "    '''\n",
        "    Computes the softmax activation of the inputs Z\n",
        "    Estimates the cross entropy loss\n",
        "    Inputs: \n",
        "        Z - numpy.ndarray (n, m)\n",
        "        Y - numpy.ndarray (1, m) of labels\n",
        "            when y=[] loss is set to []\n",
        "    \n",
        "    Returns:\n",
        "        A - numpy.ndarray (n, m) of softmax activations\n",
        "        cache -  a dictionary to store the activations later used to estimate derivatives\n",
        "        loss - cost of prediction\n",
        "    '''\n",
        "    ### CODE HERE \n",
        "    # print Z.shape\n",
        "    row = Y.shape\n",
        "    n,m = Z.shape\n",
        "    Zmax = np.zeros((m,1))\n",
        "    for i in range(0,m):    \t\n",
        "    \tZmax[i] = (np.amax(Z.T[i]))\n",
        "        \n",
        "    diff = Z.T - Zmax\n",
        "    ediff = np.exp(diff)\n",
        "    \n",
        "    A = np.zeros((m,n))\n",
        "    \n",
        "    for i in range(0,m):\n",
        "    \tvar = ediff[i]/np.sum(ediff[i])\n",
        "    \tA[i] = var\n",
        "    \n",
        "    # Calculating Loss now\n",
        "    loss = 0\n",
        "    if row[0] != 0:\n",
        "    \tloss = computeLoss(A,Y)\n",
        "\n",
        "    #added by me could be wrong\n",
        "    cache = {}\n",
        "    cache[\"A\"] = A\n",
        "    return A.T, cache, loss"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVRlbGVBpFz7"
      },
      "source": [
        "#### `softmax_cross_entropy_loss_der`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA_UCG2V-sv-"
      },
      "source": [
        "def softmax_cross_entropy_loss_der(Y, cache):\n",
        "    '''\n",
        "    Computes the derivative of softmax activation and cross entropy loss\n",
        "    Inputs: \n",
        "        Y - numpy.ndarray (1, m) of labels\n",
        "        cache -  a dictionary with cached activations A of size (n,m)\n",
        "    Returns:\n",
        "        dZ - numpy.ndarray (n, m) derivative for the previous layer\n",
        "    '''\n",
        "    ### CODE HERE \n",
        "    \n",
        "    cache[\"A\"] = cache[\"A\"].T\n",
        "    \n",
        "    #computing one_hot representation\n",
        "    Y = Y.T\n",
        "    o_h = np.zeros((4200,10))\n",
        "    for i in range(0,4200):\n",
        "        temp = np.zeros((10,))\t    \n",
        "        temp[int(Y[i][0])] = 1\n",
        "        o_h[i] = temp\n",
        "\n",
        "    o_h = o_h.T        \n",
        "    dZ = cache[\"A\"]-o_h\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5kBOO73_6ie"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEB6lW0lpBhb"
      },
      "source": [
        "#### `initialize_multilayer_weights`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNM3do2x-7aV"
      },
      "source": [
        "def initialize_multilayer_weights(net_dims):\n",
        "    '''\n",
        "    Initializes the weights of the multilayer network\n",
        "    Inputs: \n",
        "        net_dims - tuple of network dimensions\n",
        "    Returns:\n",
        "        dictionary of parameters\n",
        "    '''\n",
        "    np.random.seed(0)\n",
        "    numLayers = len(net_dims)\n",
        "    parameters = {}\n",
        "    for l in range(numLayers-1):\n",
        "        parameters[\"W\"+str(l+1)] = np.random.randn(net_dims[l+1],net_dims[l])*0.001 #CODE HERE\n",
        "        parameters[\"Wb\"+str(l+1)] = np.zeros((net_dims[l+1],net_dims[l]))\n",
        "        \n",
        "        parameters[\"b\"+str(l+1)] = np.zeros((net_dims[l+1],1)) #CODE HERE\n",
        "        parameters[\"bb\"+str(l+1)] = np.zeros((net_dims[l+1],1))\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KElkJiJv_zA7"
      },
      "source": [
        "### Forward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-JIqDHFo_CP"
      },
      "source": [
        "#### `linear_forward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsDtF_OG_FO6"
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    '''\n",
        "    Input A propagates through the layer \n",
        "    Z = WA + b is the output of this layer. \n",
        "    Inputs: \n",
        "        A - numpy.ndarray (n,m) the input to the layer\n",
        "        W - numpy.ndarray (n_out, n) the weights of the layer\n",
        "        b - numpy.ndarray (n_out, 1) the bias of the layer\n",
        "    Returns:\n",
        "        Z = WA + b, where Z is the numpy.ndarray (n_out, m) dimensions\n",
        "        cache - a dictionary containing the inputs A\n",
        "    '''\n",
        "    ### CODE HERE\n",
        "    Arow,Acol=A.shape\n",
        "\n",
        "    Z = np.dot(A.T,W.T) + b.T\n",
        "    Z = Z.T\n",
        "\n",
        "    cache = {}\n",
        "    cache[\"A\"] = A\n",
        "    return Z, cache"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfLfQlNDo81e"
      },
      "source": [
        "#### `layer_forward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJihpmK2_FIr"
      },
      "source": [
        "def layer_forward(A_prev, W, b, activation):\n",
        "    '''\n",
        "    Input A_prev propagates through the layer and the activation\n",
        "    Inputs: \n",
        "        A_prev - numpy.ndarray (n,m) the input to the layer\n",
        "        W - numpy.ndarray (n_out, n) the weights of the layer\n",
        "        b - numpy.ndarray (n_out, 1) the bias of the layer\n",
        "        activation - is the string that specifies the activation function\n",
        "    Returns:\n",
        "        A = g(Z), where Z = WA + b, where Z is the numpy.ndarray (n_out, m) \n",
        "          dimensions, and g is the the activation function\n",
        "        cache - a dictionary containing the cache from the linear and the \n",
        "          nonlinear propagation to be used for derivative\n",
        "    '''\n",
        "    Z, lin_cache = linear_forward(A_prev, W, b)\n",
        "    if activation == \"relu\":\n",
        "        A, act_cache = relu(Z)        \n",
        "    elif activation == \"linear\":\n",
        "        A, act_cache = linear(Z) \n",
        "    elif activation == \"sigmoid\":\n",
        "        A, act_cache = sigmoid(Z)\n",
        "    elif activation == \"tanh\":\n",
        "        A, act_cache = tanh(Z)\n",
        "    \n",
        "    cache = {}\n",
        "    cache[\"lin_cache\"] = lin_cache\n",
        "    cache[\"act_cache\"] = act_cache\n",
        "    return A, cache"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VMcX_6Ro55r"
      },
      "source": [
        "#### `multi_layer_forward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1XG5WcB_9lc"
      },
      "source": [
        "def multi_layer_forward(X, parameters, activation=\"relu\"):\n",
        "    '''\n",
        "    Forward propagation through the layers of the network\n",
        "    Inputs: \n",
        "        X - numpy.ndarray (n,m) with n features and m samples\n",
        "        parameters - dictionary of network parameters \n",
        "           {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
        "    Returns:\n",
        "        AL - numpy.ndarray (c,m)  - outputs of the last fully connected layer \n",
        "          before softmax, where c is number of categories and m is number of \n",
        "          samples in the batch\n",
        "        caches - a dictionary of associated caches of parameters and network \n",
        "          inputs\n",
        "    '''\n",
        "    L = len(parameters)//4  \n",
        "    A = X\n",
        "    Ab = A\n",
        "    \n",
        "    caches = []\n",
        "    for l in range(1,L):  # since there is no W0 and b0\n",
        "        parameters[\"Wb\"+str(l)],parameters[\"bb\"+str(l)] = Binarize_Stochastic(\n",
        "            parameters[\"W\"+str(l)],parameters[\"b\"+str(l)])\n",
        "        A, cache = layer_forward(A,parameters[\"Wb\"+str(l)], \n",
        "                                 parameters[\"bb\"+str(l)], activation)\n",
        "        #in_training_mode = tf.placeholder(tf.float64) ## ?\n",
        "        #Ak = keras.layers.BatchNormalization()      ## ?   \n",
        "        caches.append(cache)\n",
        "        \n",
        "    AL, cache = layer_forward(A, parameters[\"W\"+str(L)], \n",
        "                              parameters[\"b\"+str(L)], \"linear\")\n",
        "    caches.append(cache)\n",
        "    return AL, caches"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIrQYDQ4DOLN"
      },
      "source": [
        "### Backward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfFwOEk_o25v"
      },
      "source": [
        "#### `linear_backward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnYXFP67DRXl"
      },
      "source": [
        "def linear_backward(dZ, cache, W, b):\n",
        "    '''\n",
        "    Backward prpagation through the linear layer\n",
        "    Inputs:\n",
        "        dZ - numpy.ndarray (n,m) derivative dL/dz \n",
        "        cache - a dictionary containing the inputs A, for the linear layer\n",
        "            where Z = WA + b,    \n",
        "            Z is (n,m); W is (n,p); A is (p,m); b is (n,1)\n",
        "        W - numpy.ndarray (n,p)\n",
        "        b - numpy.ndarray (n, 1)\n",
        "    Returns:\n",
        "        dA_prev - numpy.ndarray (p,m) the derivative to the previous layer\n",
        "        dW - numpy.ndarray (n,p) the gradient of W \n",
        "        db - numpy.ndarray (n, 1) the gradient of b\n",
        "    '''\n",
        "    A_prev = cache[\"A\"]\n",
        "    ## CODE HERE\n",
        "    dW = np.dot(dZ, A_prev.T)    \n",
        "    db = np.sum(dZ, axis=1, keepdims=True)        \n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVaVtODNo0qu"
      },
      "source": [
        "#### `layer_backward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrCCb3xaD1hv"
      },
      "source": [
        "def layer_backward(dA, cache, W, b, activation):\n",
        "    '''\n",
        "    Backward propagation through the activation and linear layer\n",
        "    Inputs:\n",
        "        dA - numpy.ndarray (n,m) the derivative to the previous layer\n",
        "        cache - dictionary containing the linear_cache and the activation_cache\n",
        "        activation - activation of the layer\n",
        "        W - numpy.ndarray (n,p)\n",
        "        b - numpy.ndarray (n, 1)\n",
        "    \n",
        "    Returns:\n",
        "        dA_prev - numpy.ndarray (p,m) the derivative to the previous layer\n",
        "        dW - numpy.ndarray (n,p) the gradient of W \n",
        "        db - numpy.ndarray (n, 1) the gradient of b\n",
        "    '''\n",
        "    lin_cache = cache[\"lin_cache\"]\n",
        "    act_cache = cache[\"act_cache\"]\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        dZ = sigmoid_der(dA, act_cache)\n",
        "    elif activation == \"tanh\":\n",
        "        dZ = tanh_der(dA, act_cache)\n",
        "    elif activation == \"relu\":\n",
        "        dZ = relu_der(dA, act_cache)\n",
        "    elif activation == \"linear\":\n",
        "        dZ = linear_der(dA, act_cache)\n",
        "    dA_prev, dW, db = linear_backward(dZ, lin_cache, W, b)\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubfuBhfuowGZ"
      },
      "source": [
        "#### `multi_layer_backward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_imiDMLEF0M"
      },
      "source": [
        "def multi_layer_backward(dAL, caches, parameters, activation=\"relu\"):\n",
        "    '''\n",
        "    Back propgation through the layers of the network (except softmax \n",
        "    cross entropy) softmax_cross_entropy can be handled separately\n",
        "    Inputs: \n",
        "        dAL - numpy.ndarray (n,m) derivatives from the softmax_cross_entropy \n",
        "          layer\n",
        "        caches - a dictionary of associated caches of parameters and network \n",
        "          inputs\n",
        "        parameters - dictionary of network parameters \n",
        "          {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
        "    Returns:\n",
        "        gradients - dictionary of gradient of network parameters \n",
        "            {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...}\n",
        "    '''\n",
        "    L = len(caches)  # with one hidden layer, L = 2\n",
        "    gradients = {}\n",
        "    dA = dAL\n",
        "    activation = \"linear\"\n",
        "    for l in reversed(range(1,L+1)):\n",
        "        dA, gradients[\"dW\"+str(l)], gradients[\"db\"+str(l)] = \\\n",
        "              layer_backward(dA, caches[l-1], parameters[\"W\"+str(l)],\n",
        "                             parameters[\"b\"+str(l)], activation)\n",
        "        activation = activation\n",
        "    return gradients"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ThEaGZc7Efz0",
        "outputId": "6bb5ca04-0265-486b-93ff-1d3726ab4dbe"
      },
      "source": [
        "'''\n",
        "def compute_dA(A,Y):\n",
        "    #Computes dA\n",
        "    \n",
        "    Arow,Acol = Y.shape    \n",
        "    dA = -1*(np.divide(Y,A)) + np.divide((1-Y),(1-A))\n",
        "    dA = dA/Acol\n",
        "    \n",
        "    return dA\n",
        "'''"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef compute_dA(A,Y):\\n    #Computes dA\\n    \\n    Arow,Acol = Y.shape    \\n    dA = -1*(np.divide(Y,A)) + np.divide((1-Y),(1-A))\\n    dA = dA/Acol\\n    \\n    return dA\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z90trnwoBXs"
      },
      "source": [
        "### Evaluation and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqyipXCoouhy"
      },
      "source": [
        "#### `classify`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re8c4zNMEruf"
      },
      "source": [
        "def classify(X, parameters, activation):\n",
        "    '''\n",
        "    Network prediction for inputs X\n",
        "    Inputs: \n",
        "        X - numpy.ndarray (n,m) with n features and m samples\n",
        "        parameters - dictionary of network parameters \n",
        "            {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
        "    Returns:\n",
        "        YPred - numpy.ndarray (1,m) of predictions\n",
        "    '''\n",
        "    ### CODE HERE \n",
        "    # Forward propagate X using multi_layer_forward\n",
        "    # Get predictions using softmax_cross_entropy_loss\n",
        "    # Estimate the class labels using predictions\n",
        "    AL, caches = multi_layer_forward(X, parameters, activation)\n",
        "    A, cache, cost = softmax_cross_entropy_loss(AL)\n",
        "    \n",
        "    row,col = X.shape\n",
        "    YPred = np.zeros((col,1))\n",
        "    for i in range(0,col):\n",
        "    \t  YPred[i] = np.argmax(A.T[i])\n",
        "    \n",
        "    return YPred"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLDmraa6of2t"
      },
      "source": [
        "#### `update_parameters`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reJKGA3Tn8qJ"
      },
      "source": [
        "def update_parameters(parameters, gradients, epoch, learning_rate, \n",
        "                      decay_rate=0.0):\n",
        "    '''\n",
        "    Updates the network parameters with gradient descent\n",
        "    Inputs:\n",
        "        parameters - dictionary of network parameters \n",
        "            {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
        "        gradients - dictionary of gradient of network parameters \n",
        "            {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...}\n",
        "        epoch - epoch number\n",
        "        learning_rate - step size for learning\n",
        "        decay_rate - rate of decay of step size - not necessary - in case you \n",
        "        want to use\n",
        "    '''\n",
        "    alpha = learning_rate*(1/(1+decay_rate*epoch))\n",
        "    L = len(parameters)//4    \n",
        "    ### CODE HERE \n",
        "    for l in range(1,L):\n",
        "    \tparameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] \\\n",
        "                               -(alpha*gradients[\"dW\"+str(l)])\n",
        "    \tparameters[\"b\"+str(l)] = parameters[\"b\"+str(l)]\\\n",
        "                               -(alpha*gradients[\"db\"+str(l)])\n",
        "\n",
        "    return parameters, alpha"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnVk85iApvg8"
      },
      "source": [
        "#### `multi_layer_network`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HAAUzRopunr"
      },
      "source": [
        "def multi_layer_network(batch, parameters, activation, X, Y, VD, VL, net_dims, \n",
        "                        num_iterations=500, learning_rate=0.2, decay_rate=0.01):\n",
        "    '''\n",
        "    Creates the multilayer network and trains the network\n",
        "    Inputs:\n",
        "        X - numpy.ndarray (n,m) of training data\n",
        "        Y - numpy.ndarray (1,m) of training data labels\n",
        "        net_dims - tuple of layer dimensions\n",
        "        num_iterations - num of epochs to train\n",
        "        learning_rate - step size for gradient descent\n",
        "    \n",
        "    Returns:\n",
        "        costs - list of costs over training\n",
        "        parameters - dictionary of trained network parameters\n",
        "    '''\n",
        "    # parameters = initialize_multilayer_weights(net_dims)    \n",
        "    A0 = X\n",
        "    costs, Vcosts = [], []\n",
        "    \n",
        "    for ii in range(num_iterations):\n",
        "        ### CODE HERE\n",
        "        # Forward Prop\n",
        "        ## call to multi_layer_forward to get activations\n",
        "        AL, caches = multi_layer_forward(A0, parameters, activation)\n",
        "        \n",
        "        ## call to softmax cross entropy loss\n",
        "        A, cache, cost = softmax_cross_entropy_loss(AL, Y)\n",
        "        \n",
        "        # Validation Costs\n",
        "        VAL, Vcaches = multi_layer_forward(VD, parameters, activation)\n",
        "        VA, Vcache, Vcost = softmax_cross_entropy_loss(VAL, VL)\n",
        "        \n",
        "        # Backward Prop\n",
        "        ## call to softmax cross entropy loss der\n",
        "        dZ = softmax_cross_entropy_loss_der(Y, cache)\n",
        "        \n",
        "        ## call to multi_layer_backward to get gradients\n",
        "        gradients = multi_layer_backward(dZ, caches, parameters, activation)\n",
        "        \n",
        "        ## call to update the parameters\n",
        "        parameters, alpha = update_parameters(parameters, gradients, ii, \n",
        "                                              learning_rate, decay_rate)\n",
        "\n",
        "        if batch % 9 == 0 and batch!=0:\n",
        "            costs.append(cost)\n",
        "            Vcosts.append(Vcost)\n",
        "        if ii % 10 == 0:\n",
        "            print(\"Train Cost at iteration {:d} is:\".format(ii)+\\\n",
        "                  \" {:0.5f}, learning rate: {:0.5f}\\n\".format(cost, alpha)+\\\n",
        "                  \"Validation Cost at iteration {:d} is:\".format(ii)+\\\n",
        "                  \" {:0.5f}, learning rate: {:0.5f}\" .format(Vcost, alpha))\n",
        "    \n",
        "    return costs, Vcosts, parameters"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHyLGKmrYW1"
      },
      "source": [
        "#### `calculateAccuracy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig4hkYRurYsS"
      },
      "source": [
        "def calculateAccuracy(pred,label):\n",
        "    counter=0\n",
        "    for i in range(0,pred.size-1):\n",
        "\t    if pred[0][i]==label[0][i]:\n",
        "\t      counter = counter+1\n",
        "\t\n",
        "    accuracy = counter*(1.0)/pred.size\n",
        "    return accuracy"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-FqVWiarh3p"
      },
      "source": [
        "#### `train_validation_split`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YpGnxZsriP5"
      },
      "source": [
        "def train_validation_split(train_data,train_label):\n",
        "    \n",
        "    tv_data = np.split(train_data.T,[5000,6000,11000,12000,17000,18000,23000,\n",
        "                                     24000,29000,30000,35000,36000,41000,42000,\n",
        "                                     47000,48000,53000,54000,59000])\n",
        "    tvl_data = np.split(train_label.T,[5000,6000,11000,12000,17000,18000,23000,\n",
        "                                       24000,29000,30000,35000,36000,41000,42000,\n",
        "                                       47000,48000,53000,54000,59000])\n",
        "    \n",
        "    train_data = []\n",
        "    train_label = []\n",
        "    \n",
        "    train_data = tv_data[0]\n",
        "    train_label = tvl_data[0]\n",
        "    validation_data = tv_data[1]\n",
        "    validation_label = tvl_data[1]    \n",
        "    \n",
        "    for i in range(2,len(tv_data)):\n",
        "        if i%2==0:\n",
        "            train_data = np.append(train_data, tv_data[i], axis=0)\n",
        "            train_label = np.append(train_label, tvl_data[i], axis=0)\n",
        "        else:\n",
        "            validation_data = np.append(validation_data, tv_data[i], axis=0)\n",
        "            validation_label = np.append(validation_label, tvl_data[i], axis=0)\n",
        "\t    \n",
        "    \n",
        "\n",
        "    train_data = train_data.T\n",
        "    train_label = train_label.T\n",
        "    validation_data = validation_data.T\n",
        "    validation_label = validation_label.T\n",
        "    return train_data, train_label, validation_data, validation_label"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoXQUdW9r7oK"
      },
      "source": [
        "#### `main`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-_SCYJer8GX"
      },
      "source": [
        "def main(net_dims, activation):\n",
        "    '''\n",
        "    Trains a multilayer network for MNIST digit classification (all 10 digits)\n",
        "    To create a network with 1 hidden layer of dimensions 800\n",
        "    Run the progam as:\n",
        "        python deepMultiClassNetwork_starter.py \"[784,800]\"\n",
        "    The network will have the dimensions [784,800,10]\n",
        "    784 is the input size of digit images (28pix x 28pix = 784)\n",
        "    10 is the number of digits\n",
        "    To create a network with 2 hidden layers of dimensions 800 and 500\n",
        "    Run the progam as:\n",
        "        python deepMultiClassNetwork_starter.py \"[784,800,500]\"\n",
        "    The network will have the dimensions [784,800,500,10]\n",
        "    784 is the input size of digit images (28pix x 28pix = 784)\n",
        "    10 is the number of digits\n",
        "    '''\n",
        "    #net_dims = ast.literal_eval( sys.argv[1] )\n",
        "    net_dims.append(10) # Adding the digits layer with dimensionality = 10\n",
        "    print(\"Network dimensions are:\" + str(net_dims))\n",
        "\n",
        "    # getting the subset dataset from MNIST\n",
        "    train_data, train_label, test_data, test_label = \\\n",
        "            mnist(noTrSamples=50000, noTsSamples=8000,\n",
        "                  digit_range=[0,1,2,3,4,5,6,7,8,9],\n",
        "                  noTrPerClass=5000, noTsPerClass=800)    \n",
        "\n",
        "    train_data, validation_data, train_label, validation_label = \\\n",
        "      train_test_split(train_data.T, train_label.T, test_size=8000,\n",
        "                       train_size=42000, random_state=42)\n",
        "    \n",
        "    learning_rate = 0.2\n",
        "    num_iterations = 1\n",
        "    epochs = 10\n",
        "    parameters = initialize_multilayer_weights(net_dims)\n",
        "            \n",
        "    process = psutil.Process(os.getpid())\n",
        "    total_time = 0\n",
        "    times, costs, Vcosts = [], [], []\n",
        "    for j in range(0,epochs):\n",
        "        start = time.time()\n",
        "        for i in range(0,10):\n",
        "            print(\"EPOCH----------------------------->= \"+str(j))\n",
        "            print(\"BATCH----------------------------->= \"+str(i))\n",
        "            mini_train_data = train_data[i*4200 : (i*4200)+4200]\n",
        "            mini_train_label = train_label[i*4200 : (i*4200)+4200]\n",
        "            mini_train_data = mini_train_data.T\n",
        "            mini_train_label = mini_train_label.T\n",
        "        \n",
        "            cost, Vcost, parameters = \\\n",
        "              multi_layer_network(i, parameters, activation,\n",
        "                                  mini_train_data, mini_train_label, \n",
        "                                  validation_data.T, validation_label.T, \n",
        "                                  net_dims, num_iterations=num_iterations, \n",
        "                                  learning_rate=learning_rate)\n",
        "\n",
        "        timetaken = time.time()-start\n",
        "        total_time = total_time+timetaken\n",
        "        print(\"TIME TAKEN=> \" + str(timetaken)+\" seconds\")\n",
        "        print(process.memory_info().rss)\n",
        "        times.append(timetaken)\n",
        "        costs.append(cost[0])\n",
        "        Vcosts.append(Vcost[0])\n",
        "        print(costs)\n",
        "        print(Vcosts)\n",
        "\n",
        "    print(\"TOTAL TIME TAKEN=\"+str(total_time))\n",
        "    # compute the accuracy for training set and testing set\n",
        "    train_Pred = classify(train_data.T, parameters, activation)\n",
        "    test_Pred = classify(test_data, parameters, activation)    \n",
        "    trAcc = calculateAccuracy(train_Pred.T, train_label.T)\n",
        "    teAcc = calculateAccuracy(test_Pred.T, test_label)\n",
        "\n",
        "    print(\"Accuracy for training set is {0:0.3f} %\".format(trAcc))\n",
        "    print(\"Accuracy for testing set is {0:0.3f} %\".format(teAcc))\n",
        "    \n",
        "    \n",
        "    print(\"MEMORY USAGE FOR TOTAL EPOCHS----->\"+\n",
        "          str((process.memory_info().rss * 0.001)/784)+\"MB\")\n",
        "    \n",
        "    ## CODE HERE to plot costs\n",
        "    #train error vs iterations here\n",
        "    x=[]\n",
        "    for i in range(0,epochs):\n",
        "    \tx.append(i)\n",
        "    print(costs)\n",
        "    print(Vcosts)\n",
        "    print(x)\n",
        "    plt.plot(x,costs)#,'ro')\n",
        "    plt.plot(x,Vcosts)#,'b^')\n",
        "    plt.show()\n",
        "    plt.plot(x,times)\n",
        "    plt.show()\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7zbJsXVcs66D",
        "outputId": "d0ac8652-b8cc-45bf-a67a-6df5ed1b81b8"
      },
      "source": [
        "parameters = main([784,800], activation=\"relu\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network dimensions are:[784, 800, 10]\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 2.33619, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.33930, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 2.02463, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.08626, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 7.94222, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 7.99930, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 6.04615, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 6.10494, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 5.26527, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 5.23932, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 5.49390, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 5.43866, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 4.88006, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 4.79890, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 4.11748, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 4.04345, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 2.45461, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.39331, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 1.07538, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.04260, learning rate: 0.20000\n",
            "TIME TAKEN=> 9.062235593795776 seconds\n",
            "3131498496\n",
            "[1.0753809140639097]\n",
            "[1.0425970508119466]\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 1.20095, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.22140, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 1.27718, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.31179, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 1.31468, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.29256, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 1.25352, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.20635, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 1.24651, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.25290, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 1.28092, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.30319, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 0.89836, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.89774, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 0.61648, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.61150, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 0.61430, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.60918, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 0.64273, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.62859, learning rate: 0.20000\n",
            "TIME TAKEN=> 9.02764892578125 seconds\n",
            "3131498496\n",
            "[1.0753809140639097, 0.6427295517187339]\n",
            "[1.0425970508119466, 0.6285904638749193]\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 0.66333, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.67385, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 0.63736, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.65078, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 0.61132, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.61256, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 0.60654, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.58726, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 0.57213, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.58241, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 0.56689, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.58537, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 0.53248, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.56080, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 0.63778, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.64047, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 0.62867, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.63749, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 0.69937, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.69601, learning rate: 0.20000\n",
            "TIME TAKEN=> 9.108265161514282 seconds\n",
            "3131498496\n",
            "[1.0753809140639097, 0.6427295517187339, 0.6993712003629883]\n",
            "[1.0425970508119466, 0.6285904638749193, 0.6960129502212934]\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 0.61383, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.62808, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 0.67861, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.70099, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 0.60467, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.59801, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 0.63909, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.61303, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 0.63530, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.62193, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 0.65183, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.66902, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 0.63994, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.64521, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 0.59017, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.59791, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 0.55124, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.56118, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 0.53164, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.53039, learning rate: 0.20000\n",
            "TIME TAKEN=> 9.044857263565063 seconds\n",
            "3131498496\n",
            "[1.0753809140639097, 0.6427295517187339, 0.6993712003629883, 0.531643537612711]\n",
            "[1.0425970508119466, 0.6285904638749193, 0.6960129502212934, 0.530393403748046]\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 0.48897, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.50595, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 0.50577, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.51772, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 0.47594, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.48095, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 0.51991, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.49591, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 0.50129, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.50266, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 0.52367, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.53566, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 0.52511, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.53841, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 0.55713, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.56363, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 0.54403, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.55640, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 0.52109, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.52168, learning rate: 0.20000\n",
            "TIME TAKEN=> 9.09708547592163 seconds\n",
            "3131498496\n",
            "[1.0753809140639097, 0.6427295517187339, 0.6993712003629883, 0.531643537612711, 0.5210907420620942]\n",
            "[1.0425970508119466, 0.6285904638749193, 0.6960129502212934, 0.530393403748046, 0.5216840571649997]\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 0.48023, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.49721, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 0.49629, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.50726, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 0.47087, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.47692, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 0.51383, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.49092, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 0.49570, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.49784, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 0.51237, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.52407, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 0.49879, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.51315, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 0.51941, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.52529, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 0.50278, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.51319, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 0.48610, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.48939, learning rate: 0.20000\n",
            "TIME TAKEN=> 9.118208408355713 seconds\n",
            "3131498496\n",
            "[1.0753809140639097, 0.6427295517187339, 0.6993712003629883, 0.531643537612711, 0.5210907420620942, 0.48609546298355594]\n",
            "[1.0425970508119466, 0.6285904638749193, 0.6960129502212934, 0.530393403748046, 0.5216840571649997, 0.4893862456548193]\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 0.45640, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.47153, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 0.46343, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.47236, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 0.44150, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44998, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 0.46976, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.45200, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 0.44596, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.45102, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 0.43960, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.45191, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 0.42369, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44153, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 0.43850, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44233, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 0.43398, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43864, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 0.41818, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.42713, learning rate: 0.20000\n",
            "TIME TAKEN=> 9.144043207168579 seconds\n",
            "3131498496\n",
            "[1.0753809140639097, 0.6427295517187339, 0.6993712003629883, 0.531643537612711, 0.5210907420620942, 0.48609546298355594, 0.41818476739481203]\n",
            "[1.0425970508119466, 0.6285904638749193, 0.6960129502212934, 0.530393403748046, 0.5216840571649997, 0.4893862456548193, 0.4271278510001379]\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 0.41450, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.42789, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 0.41746, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.42499, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 0.40983, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.41984, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 0.43486, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.42109, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 0.42090, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.42540, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 0.41779, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43074, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 0.41483, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43092, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 0.43205, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43652, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 0.43231, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43767, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 0.41479, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.42552, learning rate: 0.20000\n",
            "TIME TAKEN=> 9.13898491859436 seconds\n",
            "3131498496\n",
            "[1.0753809140639097, 0.6427295517187339, 0.6993712003629883, 0.531643537612711, 0.5210907420620942, 0.48609546298355594, 0.41818476739481203, 0.4147949595369173]\n",
            "[1.0425970508119466, 0.6285904638749193, 0.6960129502212934, 0.530393403748046, 0.5216840571649997, 0.4893862456548193, 0.4271278510001379, 0.4255184643397463]\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 0.41673, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43028, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 0.42436, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43215, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 0.41878, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.42738, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 0.44904, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43480, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 0.44680, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44790, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 0.45174, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.46711, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 0.45370, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.46656, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 0.46531, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.47045, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 0.46007, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.46843, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 0.43947, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44838, learning rate: 0.20000\n",
            "TIME TAKEN=> 9.162720203399658 seconds\n",
            "3131498496\n",
            "[1.0753809140639097, 0.6427295517187339, 0.6993712003629883, 0.531643537612711, 0.5210907420620942, 0.48609546298355594, 0.41818476739481203, 0.4147949595369173, 0.4394686030821189]\n",
            "[1.0425970508119466, 0.6285904638749193, 0.6960129502212934, 0.530393403748046, 0.5216840571649997, 0.4893862456548193, 0.4271278510001379, 0.4255184643397463, 0.4483799157219509]\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 0.42870, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44306, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 0.43755, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44575, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 0.42395, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43285, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 0.45154, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43782, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 0.43833, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44120, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 0.43502, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44951, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 0.42740, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44224, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 0.43767, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.44225, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 0.43349, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.43951, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 0.41280, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 0.42374, learning rate: 0.20000\n",
            "TIME TAKEN=> 9.075761795043945 seconds\n",
            "3131498496\n",
            "[1.0753809140639097, 0.6427295517187339, 0.6993712003629883, 0.531643537612711, 0.5210907420620942, 0.48609546298355594, 0.41818476739481203, 0.4147949595369173, 0.4394686030821189, 0.4127980247671638]\n",
            "[1.0425970508119466, 0.6285904638749193, 0.6960129502212934, 0.530393403748046, 0.5216840571649997, 0.4893862456548193, 0.4271278510001379, 0.4255184643397463, 0.4483799157219509, 0.4237360676150331]\n",
            "TOTAL TIME TAKEN=90.97981095314026\n",
            "Accuracy for training set is 0.886 %\n",
            "Accuracy for testing set is 0.886 %\n",
            "MEMORY USAGE FOR TOTAL EPOCHS----->3994.258285714286MB\n",
            "[1.0753809140639097, 0.6427295517187339, 0.6993712003629883, 0.531643537612711, 0.5210907420620942, 0.48609546298355594, 0.41818476739481203, 0.4147949595369173, 0.4394686030821189, 0.4127980247671638]\n",
            "[1.0425970508119466, 0.6285904638749193, 0.6960129502212934, 0.530393403748046, 0.5216840571649997, 0.4893862456548193, 0.4271278510001379, 0.4255184643397463, 0.4483799157219509, 0.4237360676150331]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr7iu-lPAfT_"
      },
      "source": [
        "train_data, train_label, test_data, test_label = \\\n",
        "        mnist(noTrSamples=50000, noTsSamples=8000,\n",
        "              digit_range=[0,1,2,3,4,5,6,7,8,9],\n",
        "              noTrPerClass=5000, noTsPerClass=800) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUPaDqRzAh5G"
      },
      "source": [
        "print(train_data.shape)\n",
        "#print(train_data.T[0])\n",
        "print(classify(train_data.T[0:2].T,parameters))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}