{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary Neural Network - example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1aQtwaxcVZIJMbBnMVD7kg1csXEqM3HTs",
      "authorship_tag": "ABX9TyNriGAflHWP2w2AmnmCSqmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mauro-escobar/neural-networks/blob/main/Binary_Neural_Network_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnk0gAXiSwcL"
      },
      "source": [
        "Following [this example](https://github.com/jaygshah/Binary-Neural-Networks/blob/281ebc7f3e712801c55aa77cef52365ecc2d146d/binary_deterministic_stochastic.py#L284).\n",
        "\n",
        "### Imports and useful functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vcowmXuSoU1"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import psutil\n",
        "import time"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Q_h9GFpfsp"
      },
      "source": [
        "#### `hard_sigmoid`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxiGPHLgUDRH"
      },
      "source": [
        "def hard_sigmoid(x):\n",
        "    return np.clip((x+1.)/2.,0,1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he-miAfguX_w"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "datasets_dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "\n",
        "def one_hot(x, n):\n",
        "    if type(x) == list:\n",
        "        x = np.array(x)\n",
        "    x = x.flatten()\n",
        "    o_h = np.zeros((len(x), n))\n",
        "    o_h[np.arange(len(x)), x] = 1\n",
        "    return o_h\n",
        "\n",
        "'''\n",
        "def train_validation_split(noTrSamples=1000, noTsSamples=100, \\\n",
        "                        digit_range=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \\\n",
        "                        noTrPerClass=100, noTsPerClass=10, trData=[],trLabels=[]):\n",
        "    \n",
        "    tsX = np.zeros((noTsSamples, 28*28))\n",
        "    trX = np.zeros((noTrSamples, 28*28))\n",
        "    tsY = np.zeros(noTsSamples)\n",
        "    trY = np.zeros(noTrSamples)\n",
        "\n",
        "    count = 0\n",
        "    for ll in digit_range:\n",
        "        # Train data\n",
        "        idl = np.where(trLabels == ll)\n",
        "        idl = idl[0][: noTrPerClass]\n",
        "        idx = list(range(count*noTrPerClass, (count+1)*noTrPerClass))\n",
        "        trX[idx, :] = trData[idl, :]\n",
        "        trY[idx] = trLabels[idl]\n",
        "        # Test data\n",
        "        idl = np.where(tsLabels == ll)\n",
        "        idl = idl[0][: noTsPerClass]\n",
        "        idx = list(range(count*noTsPerClass, (count+1)*noTsPerClass))\n",
        "        tsX[idx, :] = tsData[idl, :]\n",
        "        tsY[idx] = tsLabels[idl]\n",
        "        count += 1\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    test_idx = np.random.permutation(tsX.shape[0])\n",
        "    tsX = tsX[test_idx,:]\n",
        "    tsY = tsY[test_idx]\n",
        "\n",
        "    trX = trX.T\n",
        "    tsX = tsX.T\n",
        "    trY = trY.reshape(1, -1)\n",
        "    tsY = tsY.reshape(1, -1)\n",
        "    return trX, trY, tsX, tsY\n",
        "'''\n",
        "\n",
        "\n",
        "def mnist(noTrSamples=1000, noTsSamples=100, \\\n",
        "          digit_range=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \\\n",
        "          noTrPerClass=100, noTsPerClass=10):\n",
        "    assert noTrSamples==noTrPerClass*len(digit_range), 'noTrSamples and noTrPerClass mismatch'\n",
        "    assert noTsSamples==noTsPerClass*len(digit_range), 'noTrSamples and noTrPerClass mismatch'\n",
        "    '''\n",
        "    data_dir = os.path.join(datasets_dir, 'mnist/')\n",
        "    fd = open(os.path.join(data_dir, 'train-images.idx3-ubyte'))\n",
        "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "    trData = loaded[16:].reshape((60000, 28*28)).astype(float)\n",
        "\n",
        "    fd = open(os.path.join(data_dir, 'train-labels.idx1-ubyte'))\n",
        "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "    trLabels = loaded[8:].reshape((60000)).astype(float)\n",
        "\n",
        "    fd = open(os.path.join(data_dir, 't10k-images.idx3-ubyte'))\n",
        "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "    tsData = loaded[16:].reshape((10000, 28*28)).astype(float)\n",
        "\n",
        "    fd = open(os.path.join(data_dir, 't10k-labels.idx1-ubyte'))\n",
        "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "    tsLabels = loaded[8:].reshape((10000)).astype(float)\n",
        "    '''\n",
        "    (trData, trLabels), (tsData, tsLabels) = tf.keras.datasets.mnist.load_data()\n",
        "    trData = trData.reshape((60000, 28*28)).astype(float)\n",
        "    trLabels = trLabels.reshape((60000)).astype(float)\n",
        "    tsData = tsData.reshape((10000, 28*28)).astype(float)\n",
        "    tsLabels = tsLabels.reshape((10000)).astype(float)\n",
        "\n",
        "    trData = trData/255.\n",
        "    tsData = tsData/255.\n",
        "\n",
        "    tsX = np.zeros((noTsSamples, 28*28))\n",
        "    trX = np.zeros((noTrSamples, 28*28))\n",
        "    tsY = np.zeros(noTsSamples)\n",
        "    trY = np.zeros(noTrSamples)\n",
        "\n",
        "    count = 0\n",
        "    for ll in digit_range:\n",
        "        # Train data\n",
        "        idl = np.where(trLabels == ll)\n",
        "        idl = idl[0][: noTrPerClass]\n",
        "        idx = list(range(count*noTrPerClass, (count+1)*noTrPerClass))\n",
        "        trX[idx, :] = trData[idl, :]\n",
        "        trY[idx] = trLabels[idl]\n",
        "        # Test data\n",
        "        idl = np.where(tsLabels == ll)\n",
        "        idl = idl[0][: noTsPerClass]\n",
        "        idx = list(range(count*noTsPerClass, (count+1)*noTsPerClass))\n",
        "        tsX[idx, :] = tsData[idl, :]\n",
        "        tsY[idx] = tsLabels[idl]\n",
        "        count += 1\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    test_idx = np.random.permutation(tsX.shape[0])\n",
        "    tsX = tsX[test_idx,:]\n",
        "    tsY = tsY[test_idx]\n",
        "\n",
        "    trX = trX.T\n",
        "    tsX = tsX.T\n",
        "    trY = trY.reshape(1, -1)\n",
        "    tsY = tsY.reshape(1, -1)\n",
        "    return trX, trY, tsX, tsY"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aab3SCqST03E"
      },
      "source": [
        "### Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U29F5O0apduE"
      },
      "source": [
        "#### `relu`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr09zpPoTNo0"
      },
      "source": [
        "def relu(Z):\n",
        "    '''\n",
        "    computes relu activation of Z\n",
        "    Inputs: \n",
        "        Z is a numpy.ndarray (n, m)\n",
        "    Returns: \n",
        "        A is activation. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}\n",
        "    '''\n",
        "    A = np.maximum(0,Z)\n",
        "    cache = {}\n",
        "    cache[\"Z\"] = Z\n",
        "    return A, cache"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdMnKtwVpbw7"
      },
      "source": [
        "#### `relu_der`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAdYnjpR3pb_"
      },
      "source": [
        "def relu_der(dA, cache):\n",
        "    '''\n",
        "    computes derivative of relu activation\n",
        "    Inputs: \n",
        "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}, where Z was the input \n",
        "        to the activation layer during forward propagation\n",
        "    Returns: \n",
        "        dZ is the derivative. numpy.ndarray (n,m)\n",
        "    '''\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    Z = cache[\"Z\"]\n",
        "    dZ[Z<0] = 0\n",
        "    return dZ"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMD7YqAxpZx0"
      },
      "source": [
        "#### `linear`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWxkeyTj35Yr"
      },
      "source": [
        "def linear(Z):\n",
        "    '''\n",
        "    computes linear activation of Z\n",
        "    This function is implemented for completeness\n",
        "    Inputs: \n",
        "        Z is a numpy.ndarray (n, m)\n",
        "    Returns: \n",
        "        A is activation. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}\n",
        "    '''\n",
        "    A = Z\n",
        "    cache = {}\n",
        "    return A, cache"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jM111YipXlR"
      },
      "source": [
        "#### `linear_der`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIJavsSt5NKp"
      },
      "source": [
        "def linear_der(dA, cache):\n",
        "    '''\n",
        "    computes derivative of linear activation\n",
        "    This function is implemented for completeness\n",
        "    Inputs: \n",
        "        dA is the derivative from subsequent layer. numpy.ndarray (n, m)\n",
        "        cache is a dictionary with {\"Z\", Z}, where Z was the input \n",
        "        to the activation layer during forward propagation\n",
        "    Returns: \n",
        "        dZ is the derivative. numpy.ndarray (n,m)\n",
        "    '''\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    return dZ"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLtiN7HQT6ev"
      },
      "source": [
        "### Binarizing functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCGwU5ILpTOB"
      },
      "source": [
        "#### `Binarize_Deterministic`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmseIMfQTYdr"
      },
      "source": [
        "def Binarize_Deterministic(W,b):\n",
        "    '''\n",
        "    Input: activation of Z and Binarizing Weights\n",
        "    Output: Binarized weights and activations\n",
        "    '''\n",
        "    threshold,upper,lower=0,1,-1\n",
        "    \n",
        "    Wb = np.zeros(W.shape)\n",
        "    bb = np.zeros(b.shape)\n",
        "    Wb[W>=threshold] = upper\n",
        "    Wb[W<threshold] = lower\n",
        "    bb[b>=threshold] = upper\n",
        "    bb[b<threshold] = lower\n",
        "    \n",
        "    return Wb,bb"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIeKC_z3pRO5"
      },
      "source": [
        "#### `Binarize_Deterministic_A`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KvH4R9ETo-9"
      },
      "source": [
        "def Binarize_Deterministic_A(A):\n",
        "    '''\n",
        "    Input: activation A \n",
        "    Output: Binarized A\n",
        "    '''\n",
        "    threshold,upper,lower=0,1,-1\n",
        "    \n",
        "    # A=Z\n",
        "    Ab = np.zeros(A.shape)\t\n",
        "    Ab[A>=threshold] = upper\n",
        "    Ab[A<threshold] = lower\n",
        "    \n",
        "    return Ab"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muQjKikBpO_b"
      },
      "source": [
        "#### `Binarize_Stochastic`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzx_CMsSoFfm"
      },
      "source": [
        "def Binarize_Stochastic(W,b):\t\n",
        "    #srng = RandomStreams(lasagne.random.get_rng().randint(1, 21))\n",
        "    Wb = hard_sigmoid(W/1.0)\n",
        "    #print(Wb.shape)\n",
        "    Wb[Wb>=0.5] = 1\n",
        "    Wb[Wb<0.5] = -1\t\n",
        "    bb = hard_sigmoid(b/1.0)\n",
        "      \n",
        "    return Wb,bb"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbAysLUH5axZ"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie-3HGi9pLl-"
      },
      "source": [
        "#### `computeLoss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmBukYvU5gcN"
      },
      "source": [
        "def computeLoss(A,Y):\n",
        "    # Calculating one hot vecs\n",
        "    \n",
        "    Y = Y.T\n",
        "    row,col = Y.shape\n",
        "    \n",
        "    # Computing one_hot representation\n",
        "    o_h = np.zeros((row,10))\n",
        "    for i in range(0,row):\n",
        "        temp = np.zeros((10,))\n",
        "        #print Y[i][0]\n",
        "        temp[int(Y[i][0])] = 1\n",
        "        o_h[i] = temp\n",
        "    \n",
        "    A = np.log(A)\n",
        "    \n",
        "    total_cost = -1*np.multiply(o_h,A)\n",
        "    \n",
        "    avg_loss = np.sum(total_cost)/row\n",
        "    \n",
        "    return avg_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjtwszlCpKS4"
      },
      "source": [
        "#### `softmax_cross_entropy_loss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w7m3Ewq9GCk"
      },
      "source": [
        "def softmax_cross_entropy_loss(Z, Y=np.array([])):\n",
        "    '''\n",
        "    Computes the softmax activation of the inputs Z\n",
        "    Estimates the cross entropy loss\n",
        "    Inputs: \n",
        "        Z - numpy.ndarray (n, m)\n",
        "        Y - numpy.ndarray (1, m) of labels\n",
        "            when y=[] loss is set to []\n",
        "    \n",
        "    Returns:\n",
        "        A - numpy.ndarray (n, m) of softmax activations\n",
        "        cache -  a dictionary to store the activations later used to estimate derivatives\n",
        "        loss - cost of prediction\n",
        "    '''\n",
        "    ### CODE HERE \n",
        "    # print Z.shape\n",
        "    row = Y.shape\n",
        "    n,m = Z.shape\n",
        "    Zmax = np.zeros((m,1))\n",
        "    for i in range(0,m):    \t\n",
        "    \tZmax[i] = (np.amax(Z.T[i]))\n",
        "        \n",
        "    diff = Z.T - Zmax\n",
        "    ediff = np.exp(diff)\n",
        "    \n",
        "    A = np.zeros((m,n))\n",
        "    \n",
        "    for i in range(0,m):\n",
        "    \tvar = ediff[i]/np.sum(ediff[i])\n",
        "    \tA[i] = var\n",
        "    \n",
        "    # Calculating Loss now\n",
        "    loss = 0\n",
        "    if row[0] != 0:\n",
        "    \tloss = computeLoss(A,Y)\n",
        "\n",
        "    #added by me could be wrong\n",
        "    cache = {}\n",
        "    cache[\"A\"] = A\n",
        "    return A.T, cache, loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVRlbGVBpFz7"
      },
      "source": [
        "#### `softmax_cross_entropy_loss_der`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA_UCG2V-sv-"
      },
      "source": [
        "def softmax_cross_entropy_loss_der(Y, cache):\n",
        "    '''\n",
        "    Computes the derivative of softmax activation and cross entropy loss\n",
        "    Inputs: \n",
        "        Y - numpy.ndarray (1, m) of labels\n",
        "        cache -  a dictionary with cached activations A of size (n,m)\n",
        "    Returns:\n",
        "        dZ - numpy.ndarray (n, m) derivative for the previous layer\n",
        "    '''\n",
        "    ### CODE HERE \n",
        "    \n",
        "    cache[\"A\"] = cache[\"A\"].T\n",
        "    \n",
        "    #computing one_hot representation\n",
        "    Y = Y.T\n",
        "    o_h = np.zeros((4200,10))\n",
        "    for i in range(0,4200):\n",
        "        temp = np.zeros((10,))\t    \n",
        "        temp[int(Y[i][0])] = 1\n",
        "        o_h[i] = temp\n",
        "\n",
        "    o_h = o_h.T        \n",
        "    dZ = cache[\"A\"]-o_h\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5kBOO73_6ie"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEB6lW0lpBhb"
      },
      "source": [
        "#### `initialize_multilayer_weights`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNM3do2x-7aV"
      },
      "source": [
        "def initialize_multilayer_weights(net_dims):\n",
        "    '''\n",
        "    Initializes the weights of the multilayer network\n",
        "    Inputs: \n",
        "        net_dims - tuple of network dimensions\n",
        "    Returns:\n",
        "        dictionary of parameters\n",
        "    '''\n",
        "    np.random.seed(0)\n",
        "    numLayers = len(net_dims)\n",
        "    parameters = {}\n",
        "    for l in range(numLayers-1):\n",
        "        parameters[\"W\"+str(l+1)] = np.random.randn(net_dims[l+1],net_dims[l])*0.001 #CODE HERE\n",
        "        parameters[\"Wb\"+str(l+1)] = np.zeros((net_dims[l+1],net_dims[l]))\n",
        "        \n",
        "        parameters[\"b\"+str(l+1)] = np.zeros((net_dims[l+1],1)) #CODE HERE\n",
        "        parameters[\"bb\"+str(l+1)] = np.zeros((net_dims[l+1],1))\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KElkJiJv_zA7"
      },
      "source": [
        "### Forward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-JIqDHFo_CP"
      },
      "source": [
        "#### `linear_forward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsDtF_OG_FO6"
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    '''\n",
        "    Input A propagates through the layer \n",
        "    Z = WA + b is the output of this layer. \n",
        "    Inputs: \n",
        "        A - numpy.ndarray (n,m) the input to the layer\n",
        "        W - numpy.ndarray (n_out, n) the weights of the layer\n",
        "        b - numpy.ndarray (n_out, 1) the bias of the layer\n",
        "    Returns:\n",
        "        Z = WA + b, where Z is the numpy.ndarray (n_out, m) dimensions\n",
        "        cache - a dictionary containing the inputs A\n",
        "    '''\n",
        "    ### CODE HERE\n",
        "    Arow,Acol=A.shape\n",
        "\n",
        "    Z = np.dot(A.T,W.T) + b.T\n",
        "    Z = Z.T\n",
        "\n",
        "    cache = {}\n",
        "    cache[\"A\"] = A\n",
        "    return Z, cache"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfLfQlNDo81e"
      },
      "source": [
        "#### `layer_forward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJihpmK2_FIr"
      },
      "source": [
        "def layer_forward(A_prev, W, b, activation):\n",
        "    '''\n",
        "    Input A_prev propagates through the layer and the activation\n",
        "    Inputs: \n",
        "        A_prev - numpy.ndarray (n,m) the input to the layer\n",
        "        W - numpy.ndarray (n_out, n) the weights of the layer\n",
        "        b - numpy.ndarray (n_out, 1) the bias of the layer\n",
        "        activation - is the string that specifies the activation function\n",
        "    Returns:\n",
        "        A = g(Z), where Z = WA + b, where Z is the numpy.ndarray (n_out, m) \n",
        "          dimensions, and g is the the activation function\n",
        "        cache - a dictionary containing the cache from the linear and the \n",
        "          nonlinear propagation to be used for derivative\n",
        "    '''\n",
        "    Z, lin_cache = linear_forward(A_prev, W, b)\n",
        "    if activation == \"relu\":\n",
        "        A, act_cache = relu(Z)        \n",
        "    elif activation == \"linear\":\n",
        "        A, act_cache = linear(Z)\n",
        "    \n",
        "    cache = {}\n",
        "    cache[\"lin_cache\"] = lin_cache\n",
        "    cache[\"act_cache\"] = act_cache\n",
        "    return A, cache"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VMcX_6Ro55r"
      },
      "source": [
        "#### `multi_layer_forward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1XG5WcB_9lc"
      },
      "source": [
        "def multi_layer_forward(X, parameters):\n",
        "    '''\n",
        "    Forward propagation through the layers of the network\n",
        "    Inputs: \n",
        "        X - numpy.ndarray (n,m) with n features and m samples\n",
        "        parameters - dictionary of network parameters \n",
        "           {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
        "    Returns:\n",
        "        AL - numpy.ndarray (c,m)  - outputs of the last fully connected layer \n",
        "          before softmax, where c is number of categories and m is number of \n",
        "          samples in the batch\n",
        "        caches - a dictionary of associated caches of parameters and network \n",
        "          inputs\n",
        "    '''\n",
        "    L = len(parameters)//4  \n",
        "    A = X\n",
        "    Ab = A\n",
        "    \n",
        "    caches = []\n",
        "    for l in range(1,L):  # since there is no W0 and b0\n",
        "        parameters[\"Wb\"+str(l)],parameters[\"bb\"+str(l)] = Binarize_Stochastic(\n",
        "            parameters[\"W\"+str(l)],parameters[\"b\"+str(l)])\n",
        "        A, cache = layer_forward(A,parameters[\"Wb\"+str(l)], \n",
        "                                 parameters[\"bb\"+str(l)], \"relu\")\n",
        "        #in_training_mode = tf.placeholder(tf.float64) ## ?\n",
        "        #Ak = keras.layers.BatchNormalization()      ## ?   \n",
        "        caches.append(cache)\n",
        "        \n",
        "    AL, cache = layer_forward(A, parameters[\"W\"+str(L)], \n",
        "                              parameters[\"b\"+str(L)], \"linear\")\n",
        "    caches.append(cache)\n",
        "    return AL, caches"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIrQYDQ4DOLN"
      },
      "source": [
        "### Backward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfFwOEk_o25v"
      },
      "source": [
        "#### `linear_backward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnYXFP67DRXl"
      },
      "source": [
        "def linear_backward(dZ, cache, W, b):\n",
        "    '''\n",
        "    Backward prpagation through the linear layer\n",
        "    Inputs:\n",
        "        dZ - numpy.ndarray (n,m) derivative dL/dz \n",
        "        cache - a dictionary containing the inputs A, for the linear layer\n",
        "            where Z = WA + b,    \n",
        "            Z is (n,m); W is (n,p); A is (p,m); b is (n,1)\n",
        "        W - numpy.ndarray (n,p)\n",
        "        b - numpy.ndarray (n, 1)\n",
        "    Returns:\n",
        "        dA_prev - numpy.ndarray (p,m) the derivative to the previous layer\n",
        "        dW - numpy.ndarray (n,p) the gradient of W \n",
        "        db - numpy.ndarray (n, 1) the gradient of b\n",
        "    '''\n",
        "    A_prev = cache[\"A\"]\n",
        "    ## CODE HERE\n",
        "    dW = np.dot(dZ, A_prev.T)    \n",
        "    db = np.sum(dZ, axis=1, keepdims=True)        \n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVaVtODNo0qu"
      },
      "source": [
        "#### `layer_backward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrCCb3xaD1hv"
      },
      "source": [
        "def layer_backward(dA, cache, W, b, activation):\n",
        "    '''\n",
        "    Backward propagation through the activation and linear layer\n",
        "    Inputs:\n",
        "        dA - numpy.ndarray (n,m) the derivative to the previous layer\n",
        "        cache - dictionary containing the linear_cache and the activation_cache\n",
        "        activation - activation of the layer\n",
        "        W - numpy.ndarray (n,p)\n",
        "        b - numpy.ndarray (n, 1)\n",
        "    \n",
        "    Returns:\n",
        "        dA_prev - numpy.ndarray (p,m) the derivative to the previous layer\n",
        "        dW - numpy.ndarray (n,p) the gradient of W \n",
        "        db - numpy.ndarray (n, 1) the gradient of b\n",
        "    '''\n",
        "    lin_cache = cache[\"lin_cache\"]\n",
        "    act_cache = cache[\"act_cache\"]\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        dZ = sigmoid_der(dA, act_cache)\n",
        "    elif activation == \"tanh\":\n",
        "        dZ = tanh_der(dA, act_cache)\n",
        "    elif activation == \"relu\":\n",
        "        dZ = relu_der(dA, act_cache)\n",
        "    elif activation == \"linear\":\n",
        "        dZ = linear_der(dA, act_cache)\n",
        "    dA_prev, dW, db = linear_backward(dZ, lin_cache, W, b)\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubfuBhfuowGZ"
      },
      "source": [
        "#### `multi_layer_backward`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_imiDMLEF0M"
      },
      "source": [
        "def multi_layer_backward(dAL, caches, parameters):\n",
        "    '''\n",
        "    Back propgation through the layers of the network (except softmax \n",
        "    cross entropy) softmax_cross_entropy can be handled separately\n",
        "    Inputs: \n",
        "        dAL - numpy.ndarray (n,m) derivatives from the softmax_cross_entropy \n",
        "          layer\n",
        "        caches - a dictionary of associated caches of parameters and network \n",
        "          inputs\n",
        "        parameters - dictionary of network parameters \n",
        "          {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
        "    Returns:\n",
        "        gradients - dictionary of gradient of network parameters \n",
        "            {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...}\n",
        "    '''\n",
        "    L = len(caches)  # with one hidden layer, L = 2\n",
        "    gradients = {}\n",
        "    dA = dAL\n",
        "    activation = \"linear\"\n",
        "    for l in reversed(range(1,L+1)):\n",
        "        dA, gradients[\"dW\"+str(l)], gradients[\"db\"+str(l)] = \\\n",
        "              layer_backward(dA, caches[l-1], parameters[\"W\"+str(l)],\n",
        "                             parameters[\"b\"+str(l)], activation)\n",
        "        activation = \"relu\"\n",
        "    return gradients"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ThEaGZc7Efz0",
        "outputId": "a801604b-7194-44d9-c355-8c11d94a00f5"
      },
      "source": [
        "'''\n",
        "def compute_dA(A,Y):\n",
        "    #Computes dA\n",
        "    \n",
        "    Arow,Acol = Y.shape    \n",
        "    dA = -1*(np.divide(Y,A)) + np.divide((1-Y),(1-A))\n",
        "    dA = dA/Acol\n",
        "    \n",
        "    return dA\n",
        "'''"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef compute_dA(A,Y):\\n    #Computes dA\\n    \\n    Arow,Acol = Y.shape    \\n    dA = -1*(np.divide(Y,A)) + np.divide((1-Y),(1-A))\\n    dA = dA/Acol\\n    \\n    return dA\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z90trnwoBXs"
      },
      "source": [
        "### Evaluation and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqyipXCoouhy"
      },
      "source": [
        "#### `classify`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re8c4zNMEruf"
      },
      "source": [
        "def classify(X, parameters):\n",
        "    '''\n",
        "    Network prediction for inputs X\n",
        "    Inputs: \n",
        "        X - numpy.ndarray (n,m) with n features and m samples\n",
        "        parameters - dictionary of network parameters \n",
        "            {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
        "    Returns:\n",
        "        YPred - numpy.ndarray (1,m) of predictions\n",
        "    '''\n",
        "    ### CODE HERE \n",
        "    # Forward propagate X using multi_layer_forward\n",
        "    # Get predictions using softmax_cross_entropy_loss\n",
        "    # Estimate the class labels using predictions\n",
        "    AL, caches = multi_layer_forward(X, parameters)\n",
        "    A, cache, cost = softmax_cross_entropy_loss(AL)\n",
        "    \n",
        "    row,col = X.shape\n",
        "    YPred = np.zeros((col,1))\n",
        "    for i in range(0,col):\n",
        "    \t  YPred[i] = np.argmax(A.T[i])\n",
        "    \n",
        "    return YPred"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLDmraa6of2t"
      },
      "source": [
        "#### `update_parameters`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reJKGA3Tn8qJ"
      },
      "source": [
        "def update_parameters(parameters, gradients, epoch, learning_rate, \n",
        "                      decay_rate=0.0):\n",
        "    '''\n",
        "    Updates the network parameters with gradient descent\n",
        "    Inputs:\n",
        "        parameters - dictionary of network parameters \n",
        "            {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
        "        gradients - dictionary of gradient of network parameters \n",
        "            {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...}\n",
        "        epoch - epoch number\n",
        "        learning_rate - step size for learning\n",
        "        decay_rate - rate of decay of step size - not necessary - in case you \n",
        "        want to use\n",
        "    '''\n",
        "    alpha = learning_rate*(1/(1+decay_rate*epoch))\n",
        "    L = len(parameters)//4    \n",
        "    ### CODE HERE \n",
        "    for l in range(1,L):\n",
        "    \tparameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] \\\n",
        "                               -(alpha*gradients[\"dW\"+str(l)])\n",
        "    \tparameters[\"b\"+str(l)] = parameters[\"b\"+str(l)]\\\n",
        "                               -(alpha*gradients[\"db\"+str(l)])\n",
        "\n",
        "    return parameters, alpha"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnVk85iApvg8"
      },
      "source": [
        "#### `multi_layer_network`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HAAUzRopunr"
      },
      "source": [
        "def multi_layer_network(batch, parameters, X, Y, VD, VL, net_dims, \n",
        "                        num_iterations=500, learning_rate=0.2, decay_rate=0.01):\n",
        "    '''\n",
        "    Creates the multilayer network and trains the network\n",
        "    Inputs:\n",
        "        X - numpy.ndarray (n,m) of training data\n",
        "        Y - numpy.ndarray (1,m) of training data labels\n",
        "        net_dims - tuple of layer dimensions\n",
        "        num_iterations - num of epochs to train\n",
        "        learning_rate - step size for gradient descent\n",
        "    \n",
        "    Returns:\n",
        "        costs - list of costs over training\n",
        "        parameters - dictionary of trained network parameters\n",
        "    '''\n",
        "    # parameters = initialize_multilayer_weights(net_dims)    \n",
        "    A0 = X\n",
        "    costs, Vcosts = [], []\n",
        "    \n",
        "    for ii in range(num_iterations):\n",
        "        ### CODE HERE\n",
        "        # Forward Prop\n",
        "        ## call to multi_layer_forward to get activations\n",
        "        AL, caches = multi_layer_forward(A0, parameters)\n",
        "        \n",
        "        ## call to softmax cross entropy loss\n",
        "        A, cache, cost = softmax_cross_entropy_loss(AL, Y)\n",
        "        \n",
        "        # Validation Costs\n",
        "        VAL, Vcaches = multi_layer_forward(VD, parameters)\n",
        "        VA, Vcache, Vcost = softmax_cross_entropy_loss(VAL, VL)\n",
        "        \n",
        "        # Backward Prop\n",
        "        ## call to softmax cross entropy loss der\n",
        "        dZ = softmax_cross_entropy_loss_der(Y, cache)\n",
        "        \n",
        "        ## call to multi_layer_backward to get gradients\n",
        "        gradients = multi_layer_backward(dZ, caches, parameters)\n",
        "        \n",
        "        ## call to update the parameters\n",
        "        parameters, alpha = update_parameters(parameters, gradients, ii, \n",
        "                                              learning_rate, decay_rate)\n",
        "\n",
        "        if batch % 9 == 0 and batch!=0:\n",
        "            costs.append(cost)\n",
        "            Vcosts.append(Vcost)\n",
        "        if ii % 10 == 0:\n",
        "            print(\"Train Cost at iteration {:d} is:\".format(ii)+\\\n",
        "                  \" {:0.5f}, learning rate: {:0.5f}\\n\".format(cost, alpha)+\\\n",
        "                  \"Validation Cost at iteration {:d} is:\".format(ii)+\\\n",
        "                  \" {:0.5f}, learning rate: {:0.5f}\" .format(Vcost, alpha))\n",
        "    \n",
        "    return costs, Vcosts, parameters"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHyLGKmrYW1"
      },
      "source": [
        "#### `calculateAccuracy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig4hkYRurYsS"
      },
      "source": [
        "def calculateAccuracy(pred,label):\n",
        "    counter=0\n",
        "    for i in range(0,pred.size-1):\n",
        "\t    if pred[0][i]==label[0][i]:\n",
        "\t      counter = counter+1\n",
        "\t\n",
        "    accuracy = counter*(1.0)/pred.size\n",
        "    return accuracy"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-FqVWiarh3p"
      },
      "source": [
        "#### `train_validation_split`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YpGnxZsriP5"
      },
      "source": [
        "def train_validation_split(train_data,train_label):\n",
        "    \n",
        "    tv_data = np.split(train_data.T,[5000,6000,11000,12000,17000,18000,23000,\n",
        "                                     24000,29000,30000,35000,36000,41000,42000,\n",
        "                                     47000,48000,53000,54000,59000])\n",
        "    tvl_data = np.split(train_label.T,[5000,6000,11000,12000,17000,18000,23000,\n",
        "                                       24000,29000,30000,35000,36000,41000,42000,\n",
        "                                       47000,48000,53000,54000,59000])\n",
        "    \n",
        "    train_data = []\n",
        "    train_label = []\n",
        "    \n",
        "    train_data = tv_data[0]\n",
        "    train_label = tvl_data[0]\n",
        "    validation_data = tv_data[1]\n",
        "    validation_label = tvl_data[1]    \n",
        "    \n",
        "    for i in range(2,len(tv_data)):\n",
        "        if i%2==0:\n",
        "            train_data = np.append(train_data, tv_data[i], axis=0)\n",
        "            train_label = np.append(train_label, tvl_data[i], axis=0)\n",
        "        else:\n",
        "            validation_data = np.append(validation_data, tv_data[i], axis=0)\n",
        "            validation_label = np.append(validation_label, tvl_data[i], axis=0)\n",
        "\t    \n",
        "    \n",
        "\n",
        "    train_data = train_data.T\n",
        "    train_label = train_label.T\n",
        "    validation_data = validation_data.T\n",
        "    validation_label = validation_label.T\n",
        "    return train_data, train_label, validation_data, validation_label"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoXQUdW9r7oK"
      },
      "source": [
        "#### `main`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-_SCYJer8GX"
      },
      "source": [
        "def main(net_dims):\n",
        "    '''\n",
        "    Trains a multilayer network for MNIST digit classification (all 10 digits)\n",
        "    To create a network with 1 hidden layer of dimensions 800\n",
        "    Run the progam as:\n",
        "        python deepMultiClassNetwork_starter.py \"[784,800]\"\n",
        "    The network will have the dimensions [784,800,10]\n",
        "    784 is the input size of digit images (28pix x 28pix = 784)\n",
        "    10 is the number of digits\n",
        "    To create a network with 2 hidden layers of dimensions 800 and 500\n",
        "    Run the progam as:\n",
        "        python deepMultiClassNetwork_starter.py \"[784,800,500]\"\n",
        "    The network will have the dimensions [784,800,500,10]\n",
        "    784 is the input size of digit images (28pix x 28pix = 784)\n",
        "    10 is the number of digits\n",
        "    '''\n",
        "    #net_dims = ast.literal_eval( sys.argv[1] )\n",
        "    net_dims.append(10) # Adding the digits layer with dimensionality = 10\n",
        "    print(\"Network dimensions are:\" + str(net_dims))\n",
        "\n",
        "    # getting the subset dataset from MNIST\n",
        "    train_data, train_label, test_data, test_label = \\\n",
        "            mnist(noTrSamples=50000, noTsSamples=8000,\n",
        "                  digit_range=[0,1,2,3,4,5,6,7,8,9],\n",
        "                  noTrPerClass=5000, noTsPerClass=800)    \n",
        "\n",
        "    train_data, validation_data,train_label, validation_label = \\\n",
        "      train_test_split(train_data.T, train_label.T, test_size=8000,\n",
        "                       train_size=42000, random_state=42)\n",
        "    \n",
        "    learning_rate = 0.2\n",
        "    num_iterations = 1\n",
        "    epochs = 10\n",
        "    parameters = initialize_multilayer_weights(net_dims)\n",
        "            \n",
        "    process = psutil.Process(os.getpid())\n",
        "    total_time = 0\n",
        "    times, costs, Vcosts = [], [], []\n",
        "    for j in range(0,epochs):\n",
        "        start = time.time()\n",
        "        for i in range(0,10):\n",
        "            print(\"EPOCH----------------------------->= \"+str(j))\n",
        "            print(\"BATCH----------------------------->= \"+str(i))\n",
        "            mini_train_data=train_data[i*4200 : (i*4200)+4200]\n",
        "            mini_train_label=train_label[i*4200 : (i*4200)+4200]\n",
        "            mini_train_data=mini_train_data.T\n",
        "            mini_train_label=mini_train_label.T\n",
        "        \n",
        "            cost, Vcost, parameters = \\\n",
        "              multi_layer_network(i, parameters, \n",
        "                                  mini_train_data, mini_train_label, \n",
        "                                  validation_data.T, validation_label.T, \n",
        "                                  net_dims, num_iterations=num_iterations, \n",
        "                                  learning_rate=learning_rate)\n",
        "\n",
        "        timetaken = time.time()-start\n",
        "        total_time = total_time+timetaken\n",
        "        print(\"TIME TAKEN=> \" + str(timetaken)+\" seconds\")\n",
        "        print(process.memory_info().rss)\n",
        "        times.append(timetaken)\n",
        "        costs.append(cost[0])\n",
        "        Vcosts.append(Vcost[0])\n",
        "        print(costs)\n",
        "        print(Vcosts)\n",
        "\n",
        "    print(\"TOTAL TIME TAKEN=\"+str(total_time))\n",
        "    # compute the accuracy for training set and testing set\n",
        "    train_Pred = classify(train_data.T, parameters)\n",
        "    test_Pred = classify(test_data, parameters)    \n",
        "    trAcc = calculateAccuracy(train_Pred.T, train_label.T)\n",
        "    teAcc = calculateAccuracy(test_Pred.T, test_label)\n",
        "\n",
        "    print(\"Accuracy for training set is {0:0.3f} %\".format(trAcc))\n",
        "    print(\"Accuracy for testing set is {0:0.3f} %\".format(teAcc))\n",
        "    \n",
        "    \n",
        "    print(\"MEMORY USAGE FOR TOTAL EPOCHS----->\"+\n",
        "          str((process.memory_info().rss * 0.001)/784)+\"MB\")\n",
        "    \n",
        "    ## CODE HERE to plot costs\n",
        "    #train error vs iterations here\n",
        "    x=[]\n",
        "    for i in range(0,epochs):\n",
        "    \tx.append(i)\n",
        "    print(costs)\n",
        "    print(Vcosts)\n",
        "    print(x)\n",
        "    plt.plot(x,costs)#,'ro')\n",
        "    plt.plot(x,Vcosts)#,'b^')\n",
        "    plt.show()\n",
        "    plt.plot(x,times)\n",
        "    plt.show()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7zbJsXVcs66D",
        "outputId": "8de48ad0-b7ea-4187-ec91-706e0dd86d1d"
      },
      "source": [
        "main([784,800,500])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network dimensions are:[784, 800, 500, 10]\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 5.91938, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 5.92477, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Cost at iteration 0 is: nan, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: nan, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 60.94720, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 59.67872, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 15.01419, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 15.36914, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 5.64476, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 5.59635, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 3.19489, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 3.21996, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 2.24002, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.22636, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 3.18329, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 3.21261, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 2.22327, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.20514, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 0\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 3.07665, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.99783, learning rate: 0.20000\n",
            "TIME TAKEN=> 12.631496906280518 seconds\n",
            "6695370752\n",
            "[3.0766514821623896]\n",
            "[2.99783356144783]\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 2.17399, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.17101, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 2.28525, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.29268, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 2.30133, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.30120, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 2.29285, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.29146, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 2.17657, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.17601, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 9.91714, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 9.82251, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 3.51056, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 3.47134, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 2.15188, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.13724, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 2.18500, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.15574, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 1\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 2.25715, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.25570, learning rate: 0.20000\n",
            "TIME TAKEN=> 12.518409490585327 seconds\n",
            "6695370752\n",
            "[3.0766514821623896, 2.257147407817577]\n",
            "[2.99783356144783, 2.2556997055876904]\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 2.29834, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.29935, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 2.29229, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.28981, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 2.27125, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.27071, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 2.51919, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.54697, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 2.16176, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.14744, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 2.13835, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.12357, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 2.12198, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.12153, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 2.07421, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.05968, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 3.92397, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 3.90974, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 2\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 2.14427, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.15021, learning rate: 0.20000\n",
            "TIME TAKEN=> 12.519635438919067 seconds\n",
            "6695370752\n",
            "[3.0766514821623896, 2.257147407817577, 2.144272699275865]\n",
            "[2.99783356144783, 2.2556997055876904, 2.1502113223212946]\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 2.23805, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.23899, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 2.14666, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.13551, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 2.21638, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.22988, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 2.14116, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.12155, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 2.13865, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.12590, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 2.11531, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.10012, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 2.03295, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.02479, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 4.53745, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 4.55556, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 2.12509, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.09938, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 3\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 2.06937, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.07494, learning rate: 0.20000\n",
            "TIME TAKEN=> 12.481806755065918 seconds\n",
            "6695370752\n",
            "[3.0766514821623896, 2.257147407817577, 2.144272699275865, 2.0693699244459745]\n",
            "[2.99783356144783, 2.2556997055876904, 2.1502113223212946, 2.0749370796815927]\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 2.04221, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.03014, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 2.01248, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.00924, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 1.97834, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.98859, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 2.00496, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.99514, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 2.04600, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.02332, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 2.11115, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.09457, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 2.07312, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.07099, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 2.00046, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.98718, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 1.95797, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.94415, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 4\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 1.94625, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.94878, learning rate: 0.20000\n",
            "TIME TAKEN=> 12.514329195022583 seconds\n",
            "6695370752\n",
            "[3.0766514821623896, 2.257147407817577, 2.144272699275865, 2.0693699244459745, 1.9462494662814975]\n",
            "[2.99783356144783, 2.2556997055876904, 2.1502113223212946, 2.0749370796815927, 1.9487754844104237]\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 1.94684, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.95014, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 1.92997, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.93604, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 1.92584, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.93684, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 1.94247, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.94274, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 1.95492, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.94347, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 1.94515, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.92486, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 1.94820, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.93469, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 1.98627, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.97108, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 1.98040, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.97339, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 5\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 1.94357, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.94767, learning rate: 0.20000\n",
            "TIME TAKEN=> 12.525530576705933 seconds\n",
            "6695370752\n",
            "[3.0766514821623896, 2.257147407817577, 2.144272699275865, 2.0693699244459745, 1.9462494662814975, 1.9435727870930954]\n",
            "[2.99783356144783, 2.2556997055876904, 2.1502113223212946, 2.0749370796815927, 1.9487754844104237, 1.9476651720620375]\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 1.92308, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.92804, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 1.91521, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.91835, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 1.92108, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.93362, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 1.93889, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.93001, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 1.94521, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.92940, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 1.95091, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.93283, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 1.92922, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.91535, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 1.92337, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.91017, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 1.91737, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.90094, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 6\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 1.89983, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.90116, learning rate: 0.20000\n",
            "TIME TAKEN=> 12.789942264556885 seconds\n",
            "6695370752\n",
            "[3.0766514821623896, 2.257147407817577, 2.144272699275865, 2.0693699244459745, 1.9462494662814975, 1.9435727870930954, 1.8998327617717328]\n",
            "[2.99783356144783, 2.2556997055876904, 2.1502113223212946, 2.0749370796815927, 1.9487754844104237, 1.9476651720620375, 1.901159374341146]\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 1.92312, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.92258, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 1.92776, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.93388, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 1.89639, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.90821, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 1.90503, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.89860, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 1.95748, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.93507, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 1.94687, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.92780, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 1.90031, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.88828, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 1.89579, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.88647, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 1.90347, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.88456, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 7\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 1.90346, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.90389, learning rate: 0.20000\n",
            "TIME TAKEN=> 12.683941841125488 seconds\n",
            "6695370752\n",
            "[3.0766514821623896, 2.257147407817577, 2.144272699275865, 2.0693699244459745, 1.9462494662814975, 1.9435727870930954, 1.8998327617717328, 1.9034641497665106]\n",
            "[2.99783356144783, 2.2556997055876904, 2.1502113223212946, 2.0749370796815927, 1.9487754844104237, 1.9476651720620375, 1.901159374341146, 1.9038856302413365]\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 1.89574, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.89638, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 1.91268, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.92475, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 1.89927, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.91380, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 1.89165, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.88717, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 1.93527, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.91586, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 1.93587, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.91738, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 1.89465, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.88223, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 2.16506, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 2.16558, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 2.00224, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.98019, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 8\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 1.93838, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.93787, learning rate: 0.20000\n",
            "TIME TAKEN=> 12.816360712051392 seconds\n",
            "6695370752\n",
            "[3.0766514821623896, 2.257147407817577, 2.144272699275865, 2.0693699244459745, 1.9462494662814975, 1.9435727870930954, 1.8998327617717328, 1.9034641497665106, 1.938379069353343]\n",
            "[2.99783356144783, 2.2556997055876904, 2.1502113223212946, 2.0749370796815927, 1.9487754844104237, 1.9476651720620375, 1.901159374341146, 1.9038856302413365, 1.9378730350675661]\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 0\n",
            "Train Cost at iteration 0 is: 1.92945, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.92444, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 1\n",
            "Train Cost at iteration 0 is: 1.89351, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.89990, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 2\n",
            "Train Cost at iteration 0 is: 1.87321, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.88402, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 3\n",
            "Train Cost at iteration 0 is: 1.88484, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.87670, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 4\n",
            "Train Cost at iteration 0 is: 1.92650, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.90812, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 5\n",
            "Train Cost at iteration 0 is: 1.92846, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.90673, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 6\n",
            "Train Cost at iteration 0 is: 1.88809, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.87449, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 7\n",
            "Train Cost at iteration 0 is: 1.88738, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.87636, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 8\n",
            "Train Cost at iteration 0 is: 1.89343, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.87503, learning rate: 0.20000\n",
            "EPOCH----------------------------->= 9\n",
            "BATCH----------------------------->= 9\n",
            "Train Cost at iteration 0 is: 1.90379, learning rate: 0.20000\n",
            "Validation Cost at iteration 0 is: 1.90264, learning rate: 0.20000\n",
            "TIME TAKEN=> 12.624587297439575 seconds\n",
            "6695370752\n",
            "[3.0766514821623896, 2.257147407817577, 2.144272699275865, 2.0693699244459745, 1.9462494662814975, 1.9435727870930954, 1.8998327617717328, 1.9034641497665106, 1.938379069353343, 1.9037873978547295]\n",
            "[2.99783356144783, 2.2556997055876904, 2.1502113223212946, 2.0749370796815927, 1.9487754844104237, 1.9476651720620375, 1.901159374341146, 1.9038856302413365, 1.9378730350675661, 1.9026415045498366]\n",
            "TOTAL TIME TAKEN=126.10604047775269\n",
            "Accuracy for training set is 0.280 %\n",
            "Accuracy for testing set is 0.282 %\n",
            "MEMORY USAGE FOR TOTAL EPOCHS----->8540.013714285715MB\n",
            "[3.0766514821623896, 2.257147407817577, 2.144272699275865, 2.0693699244459745, 1.9462494662814975, 1.9435727870930954, 1.8998327617717328, 1.9034641497665106, 1.938379069353343, 1.9037873978547295]\n",
            "[2.99783356144783, 2.2556997055876904, 2.1502113223212946, 2.0749370796815927, 1.9487754844104237, 1.9476651720620375, 1.901159374341146, 1.9038856302413365, 1.9378730350675661, 1.9026415045498366]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3ic9ZXo8e+ZGfVqq6FmW26SbOMCpphuC4LphCQ3EMLSdrkplJCbkHDZ5+7evbt3k5BdICELCwHMJoQUSpJLaMYGDC6AjYRxr7Kabcm2RtWqc+4fGjtCSJYszeidcj7P48czv/m973s0jzRnfvUVVcUYY0z0cTkdgDHGGGdYAjDGmChlCcAYY6KUJQBjjIlSlgCMMSZKeZwO4GRkZmbqlClTnA7DGGPCyoYNGw6patbA8rBKAFOmTGH9+vVOh2GMMWFFRPYNVm5dQMYYE6UsARhjTJSyBGCMMVHKEoAxxkQpSwDGGBOlLAEYY0yUsgRgjDFRyhKAMcYMQlX5Y3ktB5s7nA4laCwBGGPMIMqrvXzndxX8x9u7nA4laCwBGGPMIJ5ZXQnAim31ROqNsywBGGPMAAeaOnjt0/3kpcVT03iUHQdbnQ4pKCwBGGPMAL9et49eVR6+fgEAK7YddDii4LAEYIwx/XR09/KbD6soK8nhzKKJzMlPZeXWeqfDCgpLAMYY08+fP6njSFsXt507BYCykhw+rmrkSFuXs4EFgSUAY4zxU1WeWV1JcU4Ki6ZlAFBWmo1P4Z3tkdcKGFECEJGnRaReRDb1K3tQRLaJyEYReVlE0oc49l4R2Swim0TkeRGJ95cXicgHIrJLRH4nIrGB+ZGMMWZ0Pth7hK37m7nl3CmICABz8tLISoljRQR2A420BbAMWDqgbDkwR1XnAjuA+wceJCL5wN3AQlWdA7iB6/0v/xh4SFWnA43A7ScdvTHGBNCy1ZWkJ8Zw7fz842Uul1BWks2qHQ109fgcjC7wRpQAVHUVcGRA2Zuq2uN/ug4oGOJwD5AgIh4gEaiTvtS6BHjBX+dZ4NqTjN0YYwKm+kg7b245wPVnTCIh1v2Z15aUZNPS2cP6yiNDHB2eAjUGcBvw2sBCVa0FfgpUAfuBJlV9E8gAvP0SSA2QP/B4ABG5Q0TWi8j6hoaGAIVrjDGf9at1+xARblo0+XOvnTcjk1iPi7cirBtozAlARB4AeoDnBnltAnANUATkAUki8vWTOb+qPqGqC1V1YVbW5+5pbIwxY9be1cNvP6zi0tk55KcnfO71xFgP50zLYMW2gxG1KnhMCUBEbgGuBG7Uwd+Vi4G9qtqgqt3AS8A5wGEg3d8tBH3dR7VjicUYY0brpY9rae7o4dZzi4asU1aSzb7D7exuaBvHyIJr1AlARJYC9wFXq2r7ENWqgLNFJNHf718GbPUni7eBL/vr3Qz8abSxGGPMaKkqy9ZUMic/lYWTJwxZb0lpDgArI2hV8EingT4PrAWKRaRGRG4HHgVSgOUiUiEij/vr5onIqwCq+gF9A70fA5/6r/eE/7Q/AL4rIrvoGxN4KnA/ljHGjMz7uw6xq76VW84pOj71czD56QmUnJISUdNBPcNXAVW9YZDiQT+wVbUOuLzf838A/mGQenuAM0cWpjFmPDyxajc7Drby4JfnnvDDMJI8s7qSzORYrpqXO2zdi0tzeOzd3TS1d5OWGDMO0QWXrQQ2xhz3h/U1vLChhlU7DzkdyrjYe6iNldvq+dpZk4nzuIetv6Q0m16f8s6OyGgFWAIwxgDQ3NHNroa+bY9/9No2fL7Ime0ylGfXVBLjFr5+1qQR1Z9XkE5GUmzEdANZAjDGALCxuglV+PLpBWzd38yfPonsiXktHd28sKGGK07NJTs1fkTHuF3C4pJs3tleT09v+K8KtgRgjAGgvKoRgL+/opTZean89I0ddPb0OhxV8PxhfQ2tnSee+jmYspJsmjt62LCvMUiRjR9LAMYYACqqvUzLSiI9MZYfXlZCrfcov15X5XRYQeHzKc+urWTBpHTmFQ66j+WQzp+ZRYxbWLEt/LuBLAEYY1BVKqq9zC/smwd//owszp+RyaMrd9Lc0e1wdIH39vZ69h1uP+lv/wDJcR7OnprBiq3hvx7AEoAxhuojRznc1sWCSX/9NvyDpSU0tnfzn+/udjCy4Fi2ppKc1Dgum3PKqI5fUpLN7oY2Kg+F96pgSwDGGMqr+/qz5/frDpmTn8bV8/J46v29HGzucCq0gNt5sIX3dh7iprMnE+Me3UdgWUnfquBw7wayBGCMobzKS3yMi5JTUj5T/r0vFNPrUx5+a6dDkQXeM2sqifW4uOHMkU39HMykjERmZCeH/bYQlgCMMVRUe5mbn45nwDfiSRmJ3HjWZH6/vppd9a0ORRc4Te3dvPRxDdfOzyMjOW5M5yorzeGDPUfCeozEEoAxUa6zp5ctdc3MnzT4bJi7lkwnIcbNg29sG+fIAu+3H1XR0e3jlnNOfvB3oLLSbHp8yns7wnfVtCUAY6Lclrpmunp9LBhiOmRGchx3XDCVNzYfDOu57z29Pv5r7T7OKprIrLzUMZ9vQWE66YkxYT0byBKAMVGuotoLMGQLAOD284rITI7jx69tC9sbory19SC13qOjmvo5GI/bxeLibN7eXk9vmG6bYQnAmChXXuXllNR4ctM+fyesY5LiPNxz8Qw+rDzCyjCd+fL06kry0xO4ZFZOwM65pCSbxvZuKqrDs2VkCcCYKNe3AGz41bDXn1FIUWYSP359W9h9491c18SHe49w8zmTcbsCt831BTOz8LgkbO8VbAnAmCh2uLWTqiPtn1kANpQYt4vvX1rMjoOtvPhxzThEFzjLVleSEOPmqwtHP/VzMGkJMZwxZSIrLQEYY8LN8f7/Ee6Hc9mcU5hXmM5Dy3fQ0R0eG8Udbu3kT5/Ucd1p+UG5iUtZaTbbD7ZQfWSoO+OGLksAxkSximovbpdwakHaiOqLCD9cWsL+pg6eXVMZ3OAC5DcfVNHV4+PWc6cE5fxlx+8VHH6tAEsAxkSx8iovxTkpJMaO6O6wACyalsHi4ix+8fYumtpDexFUd6+PX63bx/kzMpmenTL8AaNQlJnE1MyksNwWwhKAMVHK51M+qfaecPrnUO5bWkJLZw//8c6uIEQWOK9+up/6ls6gffs/pqw0m3W7D9Pa2RPU6wTasAlARJ4WkXoR2dSv7EER2SYiG0XkZRH53G+QiBSLSEW/f80i8h3/a/8oIrX9Xrt84PHGmODa3dBKS2fPkAvATqQ0N5UvLsjnmTWV1HmPBiG6wHhmdSVFmUlcNDM7qNdZUpJDV6+P98PsXsojaQEsA5YOKFsOzFHVucAO4P6BB6nqdlWdr6rzgdOBduDlflUeOva6qr46quiNMaNW7h8AHskMoMF895KZoPDQ8h2BDCtgyqsaqaj2cvOiybgCOPVzMAunTCAl3hN2q4KHTQCqugo4MqDsTVU91tZZBxQMc5oyYLeq7htVlMaYgCuv8pIS72FqZvKoji+YkMjfLJrMix/XsP1AS4CjG7tlaypJjvPwpdOH+3gauxi3i4v8q4J9YbRGIhBjALcBrw1T53rg+QFld/q7kJ4WkQlDHSgid4jIehFZ39DQMNZYjTF+xxaAjeXb8bcXTycpzsNPXg+tjeIONnfwl437+crCAlLiAz/1czBlJdkcau1iY23TuFwvEMaUAETkAaAHeO4EdWKBq4E/9Ct+DJgGzAf2A/821PGq+oSqLlTVhVlZWWMJ1xjj197Vw/YDzSOe/z+UCUmxfPOiaazYVs8Hew4HKLqxe27dPnpVuXnRlHG75kXFWbiEsOoGGnUCEJFbgCuBG/XEu0NdBnysqsffFVU9qKq9quoDngTOHG0cxpiTt7GmCZ+Ovv+/v1vPKSInNY4fvR4aG8V1dPfy3AdVLCnOZkpm0rhdNz0xloWTJ7IijFYFjyoBiMhS4D7galUdbvnbDQzo/hGR3H5Pvwhswhgzbo6tAJ5XMPYEkBDr5t6LZ1Je5eWNzc5/+31l434Ot3UFbNfPk7GkNJst+5tDemZUfyOZBvo8sBYoFpEaEbkdeBRIAZb7p3E+7q+bJyKv9js2CbgEeGnAaX8iIp+KyEZgMXBvYH4cY8xIlFc1Mjkjccx3xTrmy6cXMD07mZ+8sY2eXl9Azjkaqsozq/cyIzuZc6dnjPv1Ly7tm24aLquCRzIL6AZVzVXVGFUtUNWnVHW6qhb2m8b5DX/dOlW9vN+xbaqaoapNA855k6qeqqpzVfVqVd0f+B/NGDOUke4AOlIet4v7Li1mT0Mbv1/v3EZxH1U2srmumVvOnYJIcKd+DmZaVjKTJiZGTgIwxkSW/U1HOdjcGdAEAHDJrBxOnzyBh9/aQXuXMytil63ZS1pCDNctCP7Uz8GICGWl2azedYijXaG/WZ4lAGOiTHnVsQVgQ86+HhUR4f7LSqhv6eSZ1ZUBPfdI1HqP8sbmg1x/ZiEJse5xv/4xZSU5dPb4WL0r9FcFWwIwJspUVHuJdbsozQ385mgLp0zkklk5PP7Obo60dQX8/CfyX2srUVVuOnvyuF53oDOLJpIc52HFNucHxIdjCcCYKFNe1cjs/FTiPMH5lnzfpcW0dfXw6Mrx2yjuaFcvv/2wmktnn0LBhMRxu+5gYj0uLpiZyYqt9SExLfZELAEYE0W6e318WtsU8P7//mbkpPCV0wv51brKcbtJysvltTQd7XZk6udglpTkUN/Syea6ZqdDOSFLAMZEke0HWujo9gW8/3+g71wyA5cI/z4OG8WpKsvW7GVWbipnTAnuzzVSi4uzEIG3QnxVsCUAY6LI8R1Ag9gCAMhNS+DWc4v4Y0Utm+uCuzfOmt2H2XGwlVsdmvo5mIzkOBYUpof8dFBLAMZEkYoqLxlJsRRMSAj6tb550TRS42P4yevbg3qdZ1bvJSMplqvm5QX1OierrDSHjTVNHGzucDqUIVkCMCaKlFc3smBS+rh8U05LiOHOxdN5d0cDa4I0JXLf4TZWbKvna2dNIj7Guamfgynzrwp+O4RbAZYAjIkSTe3d7GloC+oA8EA3LZpMXlo8//ratqDsk//smn24Rfi6w1M/B1Ock0J+ekJI3yvYEoAxUaKiJjgLwE4kPsbNd79QzKe1Tby6KbA7vrR29vCH9dVcMTeXnNT4gJ47EI6tCn5/5yE6ukNzVbAlAGOiREWVFxGYW5A2rtf94oJ8Sk5J4cE3ttMdwI3iXtxQQ0tnD7ecMyVg5wy0JSXZHO3uZW0I3SuhP0sAxkSJiupGpmclj9sdso5xu4QfLC1h3+F2nv+wKiDn9PmUZWsqmV+YPq4tmpN19tQMEmPdIXuTGEsAxkQBVaWi2huQG8CMxkXFWZxVNJGfrdhJa+fYN4p7d0cDew+1ceu5U8YeXBDFx7g5b3omK0N0VbAlAGOiwL7D7TS2dzO/0JlvyyLCDy8r4VBrF798b8+Yz/fMmkqyU+K4bE7u8JUdVlaaTV1TB9sOtDgdyudYAjAmCpRXNwKBuQXkaC2YNIHL5pzCk6v20NDSOerz7KpvZdWOBm46ezKxntD/CFtc0jcdNBS7gUL/3TPGjFlFlZfEWDczcwK/A+jJ+P6lxXT0+Pj5yp2jPseyNXuJ9bj42lmTAhhZ8GSnxDOvIC0kp4NaAjAmCpRXe5lbkIbb5exWCVOzkrn+jEJ+80EVlYfaTvr4pqPdvLihlqvn5QXsdpbjYUlJDhXVXg61jr7lEwyWAIyJcB3dvWzd3+xY//9A95TNIMbt4qdvnvwWEb//qJqj3b0hP/g7UFlpNqqhtyrYEoAxEW5zXTPdvTquK4BPJDs1nr89v4hXNu5no39x2kj0+pRn11ZyZtFEZueN71qGsZqdl8opqfEhtzmcJQBjIlx5lfMDwAPdccFUJibF8qPXto14euRbWw9S03iUW0N44ddQRIQlpdms2tFAZ0/orAoeNgGIyNMiUi8im/qVPSgi20Rko4i8LCKf+80SkWIRqej3r1lEvuN/baKILBeRnf7/Q6NtakwEqqj2kpcWH1LbJaTEx3DXkums2X2YVTtHtlHcM6v3kp+ewCWzcoIcXXCUlWTT1tXLh3uPOB3KcSNpASwDlg4oWw7MUdW5wA7g/oEHqep2VZ2vqvOB04F24GX/yz8EVqjqDGCF/7kxJgjKq7whuVr2a2dNonBiAj8awUZxW/c3s27PEf5m0WQ87vDsuDhnWiZxHhcrtoZON9Cw76SqrgKODCh7U1WPLedbBxQMc5oyYLeq7vM/vwZ41v/4WeDaEUdsjBmxhpZOar1HQ6b/v784j5vvfaGYrfub+fMndSesu2x1JfExLr56RuE4RRd4CbF9q4JXbDsYMquCA5FKbwNeG6bO9cDz/Z7nqOqxrQEPAEO26UTkDhFZLyLrGxoaxhapMVGmwn8HsPkh1P/f31Vz85idl8pP39w+ZN/4kbYu/lhRy3WnFZCeGDvOEQbWktJsqo8cZVd9q9OhAGNMACLyANADPHeCOrHA1cAfBntd+1LhkOlQVZ9Q1YWqujArK2ss4RoTdcqrGvG4hDkhOmvG5erbIqKm8Si/Xjf4RnHPf1hFZ48vLAd/Byor6fuu+1aIdAONOgGIyC3AlcCNeuL2zGXAx6rafx30QRHJ9Z8nFwiNd8OYCFNR7aUkN4WE2NC6W1Z/58/I4rzpmTy6cifNHd2fea2718ev1u7jvOmZzHB4FXMgnJIWz+y8VFZuC41tIUaVAERkKXAfcLWqtg9T/QY+2/0D8GfgZv/jm4E/jSYOY8zQen3KJ9VeFoTIArAT+cHSEhrbu3ni3c9uFPf6pgMcaO4Iu4VfJ1JWks2GfY00tnU5HcqIpoE+D6wFikWkRkRuBx4FUoDl/imej/vr5onIq/2OTQIuAV4acNofAZeIyE7gYv9zY0wA7apvpa2rNyQHgAc6tSCNq+bl8cv391Df7ybqy9ZUMjkjkcXF2Q5GF1hlpTn4FN7Z4XzHx0hmAd2gqrmqGqOqBar6lKpOV9XCY9M8VfUb/rp1qnp5v2PbVDVDVZsGnPOwqpap6gxVvVhVQ2dirDERIhQXgJ3I979QTK9PeXhF30ZxG2u8bNjXyM2LpuByeA+jQDo1P42slLiQmA4anhNqjTHDqqj2kpYQQ1FmktOhjMikjERuPGsyv/uomt0NrTyzupLkOA9fWTjcLPPw4nIJS4qzeXdHQ0BvkTmqWBy9ujEmaCqqvcwrTEckfL4937lkOvEeF3//8iZe2VjHl08vGPdbWI6HJaXZtHT08FGls50flgCMiUCtnT1sP9jCgjDo/+8vMzmOOy6Yxto9h+nxKTdHwNTPwZw3PZNYt/Orgi0BGBOBNtZ4UQ3dBWAn8rfnF5GdEkdZSU7YdF+drKQ4D4umZTi+O6glAGMiUHmVfwVwQfglgKQ4D3+5+3wevn6+06EEVVlpNnsPtbGnwblVwZYAjIlAFdVeijKTmJAUnlsnZKXEkRzncTqMoFpy/F7BzrUCLAEYE2FUlYpqb1jM/49mBRMSKTklhRUOrgq2BGBMhKn1HqWhpTNs5v9HsyUl2XxU2UhTe/fwlYPAEoAxEeb4DqDWAgh5ZaU59PqUd3c6s9OxJQBjIkx5lZc4j4uSU1KdDsUMY35hOhOTYlm51ZluIEsAxkSYimovc/LTiPXYn3eoc7uExcXZvL29gR4HVgXbb4gxEaSrx8entU1htwAsmpWVZtN0tJuP/VN3x5MlAGMiyLYDzXT1+MJyAVi0On9GJjFuYYUD3UCWAIyJIDYAHH5S4mM4qyiDFQ6sCrYEYEwEKa/ykpUSR356gtOhmJOwpCSbXfWt7DvcNq7XtQRgTAQ5tgAsnHYANX3jADD+q4ItARgTIRrbuth7qM0WgIWhyRlJTM9OHvfN4SwBGBMhKmqs/z+clZVk88Hew7R0jN+qYEsAxkSI8iovIjA3DHcANX2rgrt7lfd2Hhq3a1oCMCZCVFR7Kc5JifhdNCPVaZPSSUuIGddxgGETgIg8LSL1IrKpX9mDIrJNRDaKyMsiMuhXDhFJF5EX/HW3isgif/k/ikitiFT4/10+2PHGmJHx+ZRPbAfQsOZxu1hcnMXb2+vp9em4XHMkLYBlwNIBZcuBOao6F9gB3D/EsY8Ar6tqCTAP2NrvtYdUdb7/36snF7Yxpr+9h9toOtptA8BhbklpDkfauo6v5wi2YROAqq4Cjgwoe1NVe/xP1wEFA48TkTTgAuAp/zFdqjr+a52NiQIVx+4AVjjB4UjMWFw4Iwu3a/xWBQdiDOA24LVByouABuAZESkXkV+KSP8bfN7p70J6WkTst9aYMSivbiQ5zsP07GSnQzFjkJYYwxlTJozbdNAxJQAReQDoAZ4b5GUPcBrwmKouANqAH/pfewyYBswH9gP/doJr3CEi60VkfUODM3tmGxPqKqq9zC1Iw+2yBWDhrqwkh20HWqhpbA/6tUadAETkFuBK4EZVHWzEogaoUdUP/M9foC8hoKoHVbVXVX3Ak8CZQ11HVZ9Q1YWqujArK2u04RoTsY529bJtf4sNAEeIY6uCx6MVMKoEICJLgfuAq1V10DSlqgeAahEp9heVAVv8x+f2q/pFYBPGmFHZVNdEj09ZMMl6UiPB1KxkijKTxmU66EimgT4PrAWKRaRGRG4HHgVSgOX+aZyP++vmiUj/GT13Ac+JyEb6unv+r7/8JyLyqb98MXBv4H4kY6LLXweArQUQKZaUZLN292HaOnuGrzwGw64YUdUbBil+aoi6dcDl/Z5XAAsHqXfTScRojDmB8upGCiYkkJUS53QoJkDKSrN56v29vL/rEJfOPiVo17GVwMaEuYoqWwAWac6YMpGUeA8rg9wNZAnAmDB2sLmDuqYOSwARJsbt4sKZWazYVo8viKuCLQEYE8bK/f3/NgAcecpKsznU2smntU1Bu4YlAGPCWEW1lxi3MDsv1elQTIBdNDMblxDUVcFRkQAOt3by7g5bRGYiT3lVI7NyU4mPcTsdigmwCUmxnD55QlDvFRwVCeCf/7KVb/16A0faupwOxZiA6en18Wltk/X/R7AlJTlsrmvmQFNHUM4fFQngWxdNo727lyff2+N0KMYEzI6DrbR39Vr/fwS7+Ni9grcFpxsoKhLAjJwUrpqbx7NrKjnU2ul0OMYExLEtg60FELmmZydTODEhaNNBoyIBANxdNoOO7l6eWGWtABMZKqobmZAYw+SMRKdDMUEiIpSV5PD+rkMc7eoN+PmjJgFMz07mmvn5/NfaShparBVgwl+5fwGYiO0AGsnKSrPp7PGxZnfg7xUcNQkA+loB3b3K4+/udjoUY8akuaObXQ2tdgOYKHBm0UQeuX4+ZxRNDPi5oyoBFGUmce38fH69bh/1zcEZVTdmPGysbkIVuwVkFIjzuLlmfj6p8TEBP3dUJQCAu8um0+NT/uMdawWY8FVR3QjAPBsANmMQdQlgckYSXzotn998WBW0ubXGBFt5lZdpWUmkJQT+W6GJHlGXAADuWjIDn0/5j3d2OR2KMSdNVamo9lr/vxmzqEwAhRMT+crCAn77YTV13qNOh2PMSalpPMrhti7mW/+/GaOoTAAA3148HUX5xdvWCjDh5eOqvv7/Bdb/b8YoahNAwYREvnpGIb9fX01N46C3NTYmJFVUe4mPcVFySorToZgwF7UJAPpaAYJYK8CElfIqL3Pz0/G4o/rP1wRAVP8G5aYlcMOZhfxhfQ3VR6wVYEJfZ08vW+qarf/fBERUJwCAby2ejssl/HzlTqdDMWZYW/e30NXrsw3gTEAMmwBE5GkRqReRTf3KHhSRbSKyUUReFpFBfxtFJF1EXvDX3Soii/zlE0VkuYjs9P/v2Hy2nNR4bjxrEi9+XEvloTanwjBmRMqPDQBbC8AEwEhaAMuApQPKlgNzVHUusAO4f4hjHwFeV9USYB6w1V/+Q2CFqs4AVvifO+abF07D4xJ+vtLGAkxoq6j2kpMaR25agtOhmAgwbAJQ1VXAkQFlb6pqj//pOqBg4HEikgZcADzlP6ZLVb3+l68BnvU/fha4dlTRB0h2ajw3nT2Zl8tr2NPQ6mQoxpxQeZWXBbYAzARIIMYAbgNeG6S8CGgAnhGRchH5pYgk+V/LUdX9/scHgJyhTi4id4jIehFZ39AQvPv6/vcLpxHncVsrwISsw62dVB1ptwFgEzBjSgAi8gDQAzw3yMse4DTgMVVdALQxSFePqiqgQ11DVZ9Q1YWqujArK2ss4Z5QVkocf7NoMn+qqGVXfUvQrmPMaB27A5gtADOBMuoEICK3AFcCN/o/xAeqAWpU9QP/8xfoSwgAB0Uk13+eXCB4t70/CXdcMJX4GDePrLBWgAk9FdVe3C7h1II0p0MxEWJUCUBElgL3AVer6qAT6FX1AFAtIsX+ojJgi//xn4Gb/Y9vBv40mjgCLSM5jpvPmcIrG+vYcdBaASa0VFR7mZmTQmKsx+lQTIQYyTTQ54G1QLGI1IjI7cCjQAqwXEQqRORxf908EXm13+F3Ac+JyEZgPvB//eU/Ai4RkZ3Axf7nIeGO86eSGOPmkbdsXYAJHT6fUlHltemfJqCG/SqhqjcMUvzUEHXrgMv7Pa8AFg5S7zB9LYKQMyEpllvPLeLRt3dx14FmSk5JdTokY9hzqJWWzh5bAGYCKupXAg/mb88vIiXOw8PLrRVgQsPHVX0DwKdZC8AEkCWAQaQnxnLreUW8vvkAm+uanA7HGCqqvaTEe5iamex0KCaCWAIYwu3nFZES7+FhGwswIaCiysu8gnRcLnE6FBNBLAEMIS0hhr87fyrLtxzk0xprBRjntHf1sO1Asw0Am4CzBHACt547hbSEGB5+a4fToZgo9mlNEz7FBoBNwFkCOIGU+BjuuGAqK7bVH1+Facx4K/f/7lkCMIFmCWAYN58zhQmJ1gowzqmo8jJpYiIZyXFOh2IijCWAYSTHebjjgmm8s72BDfsanQ7HRKHy6kbr/zdBYQlgBP5m0WQmJsVaK8CMu/1NRznY3GndPyYoLAGMQFKch29cOJX3dh5ifeWR4Q8wJkAqqqz/3wSPJYARuunsKWQmx/GQtQLMOCqv9hLrdjErz7YkMYFnCQXQgSUAABAlSURBVGCEEmLdfOPCqazedZgP9hx2OhwTJSqqvMzKSyXO43Y6FBOBLAGchK+fPZmsFGsFmPHR3etjY63tAGqCxxLASYiPcfOti6axbs8R1uw+5HQ4JsJtP9BCR7fP+v9N0FgCOEk3nDmJnNQ4Hlq+g8FvhGZMYPz1FpB2E3gTHJYATlJ8jJtvL57OR5WNrN5lYwEmeMqrvGQkxVI4McHpUEyEsgQwCl89o5DctHj+ffl2awWYoKmobmR+YToitgOoCQ5LAKMQ5+lrBXxc5WXVThsLMIHX1N7N7oY2GwA2QWUJYJT+28JC8tMT+HcbCzBB8EnNsQVg1v9vgscSwCjFelzctWQ6n1R7eWd7g9PhmAhTXuVFBOYWpjkdiolgwyYAEXlaROpFZFO/sgdFZJuIbBSRl0Vk0HaqiFSKyKciUiEi6/uV/6OI1PrLK0Tk8sGOD3VfOr2AwonWCjCBV1HdyPSsZFLjY5wOxUSwkbQAlgFLB5QtB+ao6lxgB3D/CY5frKrzVXXhgPKH/OXzVfXVEUccQmLcLu5aMoNPa5t4a2u90+GYCKGqVFR7bf6/CbphE4CqrgKODCh7U1V7/E/XAQVBiC0sXLcgn8kZibYuwATMvsPtNLZ3s2CS9f+b4ArEGMBtwGtDvKbAmyKyQUTuGPDanf4upKdFJGx/0z1uF3cvmcGW/c28sfmg0+GYCFBhdwAz42RMCUBEHgB6gOeGqHKeqp4GXAZ8W0Qu8Jc/BkwD5gP7gX87wTXuEJH1IrK+oSE0B1uvmZ9HUWYSD7+1A5/PWgFmbMqrGkmMdTMzJ9npUEyEG3UCEJFbgCuBG3WIvg9VrfX/Xw+8DJzpf35QVXtV1Qc8eax8iHM8oaoLVXVhVlbWaMMNKo/bxT1lM9h2oIXXNx9wOhwT5iqqvZyan4bHbZP0THCN6jdMRJYC9wFXq2r7EHWSRCTl2GPgC8Am//PcflW/eKw8nF01L49pWdYKMGPT0d3Llv3NzLcFYGYcjGQa6PPAWqBYRGpE5HbgUSAFWO6fxvm4v26eiByb0ZMDvC8inwAfAn9R1df9r/3EPz10I7AYuDewP9b4c7uEey6eyY6Drfzl0/1Oh2PC1Oa6Zrp71TaAM+PCM1wFVb1hkOKnhqhbB1zuf7wHmDdEvZtOIsawccWpuTy6ciePrNjJ5afm4nbZHi7m5BzfAdRaAGYcWCdjALldwj1lM9lV38orG+ucDseEofKqRvLS4slJjXc6FBMFLAEE2GVzTqHklBQeeWsnPb0+p8MxYaai2mv9/2bcWAIIMJdL+M7FM9hzqI0/f2KtADNyDS2d1DQetf5/M24sAQTBF2adwqzcVH62wloBZuSOLwCzFoAZJ5YAgsDlEu69ZCaVh9t5ubzW6XBMmKiobsTtEubk2Q6gZnxYAgiSi0uzOTU/jZ+t3Em3tQLMCJRXeSnNTSEh1u10KCZKWAIIEhHh3ktmUH3kKC99XON0OCbE9fqUjTVNtv+PGVeWAIJocXE28wrT+dmKXXT1WCvADG1XfSutnT02AGzGlSWAIBIR7r14BrXeo7ywwVoB5rN8PmXvoTb+3yd1/GzFTsAGgM34GnYlsBmbC2dmsWBSOo+u3MmXTs8nzmP9u9Go16fsaWjl09omNtU2s6muiS11zbR29t1WI9bt4qLiLIoykhyO1EQTSwBBJiJ895KZ3PTUh/z+o2puWjTF6ZBMkHX1+NhZ38Jm/wf9ptomtuxvpqO7rxswPsbFrNxUrjstnzl5aczOT2VGdgqxHmuQm/FlCWAcnDc9k4WTJ/CLt3fzlYWFxMcEtxXg8yldvT56fEp3j49un4/uXqWn10d3r49eHxRlJtkHTgB0dPey/UDL8Q/6TbXNbD/QQpd/5ldynIdZeal87czJzMlP5dT8NIoyk2yrZxMSLAGMg2OtgK/98gPuf+lTJmck0tOrdPf2fTB39/ro8fV73Ov/AO/3erf/A72rx//BfoJ6I9mNempmEg9+ZS6nT54Y/DcgQrR19rB1f3PfB31d3/8761vp9b/haQkxnJqfxq3nTWFOXhpz8tOYPDERl20KaEKUJYBxsmhaBhfOzDq+MMztEjwuIdbtwuMWYtwu/z/B0+9xjNuFxyUkxXnwuIau53G5iPEIMa6+Mo974Ln953K76Ojq5ZEVO/ny42u57dwivveFYpt7PkDT0W621DWzua7J32/fxJ5DbRy79VFmcixz8tO4uDSHOfmpzM5Lo2BCAiL2YW/Ch4TTjcwXLlyo69evdzqMUVNVOnt8xLpdjn8rbO3s4UevbeXX66qYkpHIj780l7OmZjgak1OOtHX5v9U3He+333f4r/c5yk2LZ3ZeGqfmpzEnP5U5+Wlkp8TZh70JGyKyQVUXfq7cEkB0W7P7ED94cSPVR45y86LJ3Le0hKS46GgY7m86yoOvb+elftt1TJqYePwb/Zz8NGbnpZKZHOdglMaM3VAJIDr+0s2QzpmWyev3XMCDb2xn2ZpKVm6v58fXzeWc6ZlOhxY0R7t6+c9Vu3n83d34fPB35xexuDib2XlppCXGOB2eMePGWgDmuA/3HuG+Fz6h8nA7N541ifsvLyU5gloDPp/yp09q+fFr2znQ3MEVp+byw8tKKJyY6HRoxgSVtQDMsM4smshr91zAv725nadW7+Wd7Q3863WncsHMLKdDG7MN+47wT/9vC5/UNPVt0nfDAs4sshlQJrpZC8AMasO+Rr7/wifsaWjjqwsLeeDKUlLjw697pKaxnR+9to1XNu4nJzWO719awnUL8h0fhDdmPFkLwJyU0ydP4NW7z+eht3bw5Ko9vLujrzWwuCTb6dBGpLWzh8fe2cWT7+1FgLvLZvCNC6eSGGu/8sYcM2wLQESeBq4E6lV1jr/sQeAqoAvYDdyqqt5Bjq0EWoBeoOdYBhKRicDvgClAJfDfVLVxuGCtBeCMimov3//DJ+ysb+VLpxXwv66cFbKDpb0+5cUNNTz45nYaWjq5dn4e9y0tIS89wenQjHHMUC2AkaxHXwYsHVC2HJijqnOBHcD9Jzh+sarOH3DxHwIrVHUGsML/3ISo+YXpvHL3edy5eDp/rKjlkofeZfmWg06H9Tlrdx/mqp+/z30vbqRgQgIvf+scHr5+gX34GzOEYROAqq4Cjgwoe1NVe/xP1wEFJ3nda4Bn/Y+fBa49yePNOIvzuPnepcX88VvnMjEplr/7r/Xc89tyGtu6nA6NfYfb+O+/Ws8NT67D297Fz25YwEvfPIcFk2xvfWNOJBAdorfR150zGAXeFBEF/lNVn/CX56jqfv/jA0DOUCcXkTuAOwAmTZoUgHDNWJxakMaf7zyPX7y9i1+8vYvVuw7xz9fOYemc3HGPpbmjm0dX7mLZ6ko8buF/XDKTv7tgatA32zMmUoxoFpCITAFeOTYG0K/8AWAhcJ0OciIRyVfVWhHJpq/b6C5VXSUiXlVN71evUVWH/bpmYwChZUtdM99/4RM21zVzxdxc/unq2WSMw6rZnl4fv/2omoeW7+BIexdfPq2A711aTE5qfNCvbUw4CvgsIBG5hb7B4bLBPvwBVLXW/3+9iLwMnAmsAg6KSK6q7heRXKB+tHEY58zKS+WP3z6Xx9/Zzc9W7mTt7sP876tnc+Xc3KDtk/Pezgb++ZWtbD/YwplFE3n2ylnMyU8LyrWMiXSj2pRcRJYC9wFXq2r7EHWSRCTl2GPgC8Am/8t/Bm72P74Z+NNo4jDOi3G7uKtsBq/cdT4FExK46/lyvvnrj2lo6QzodXY3tHL7so+46akPae/u4bEbT+N3d5xtH/7GjMFIpoE+D1wEZAIHgX+gb9ZPHHDYX22dqn5DRPKAX6rq5SIyFXjZ/7oH+I2q/ov/nBnA74FJwD76poF+ZqB5MNYFFNp6en08+d5eHnprB4mxbv7xqtlcMz9vTK0Bb3sXj6zYya/W7iM+xs2dS6ZzyzlTrJ/fmJNgu4GacbOrvoXvv7CR8iovF5dm8y9fPPWk++e7e308t24fD721k5aObr56xiS+e8lMslJsZ05jTpYlADOuen3K0+/v5advbifO4+J/XTWbL52WP2xrQFV5e3s9//KXrexuaOPc6Rn8/RWzKM1NHafIjYk8lgCMI/Y0tPKDFzfyUWUjFxVn8a/XnUpu2uALs7YfaOGf/7KF93YeoigziQcuL6WsNNtuvGLMGFkCMI7x+ZRn11by49e3EeNy8cAVpXz1jMLjH+yHWzt56K0d/OaDKpLjPNxz8UxuOnuy3bTemACxBGAct+9wG/e9sJEP9h7h/BmZ/NM1c1i+5QA/X7mL9q5evn7WJL5z8UwmJMU6HaoxEcUSgAkJPp/y3Af7+NfXttHe1QvARcVZPHB5KTNyUhyOzpjIZNtBm5Dgcgk3LZrCRcXZPLFqD2Wl2VxUHB5bTBsTaSwBGEcUTkzk/1w7Z/iKxpigsVE2Y4yJUpYAjDEmSlkCMMaYKGUJwBhjopQlAGOMiVKWAIwxJkpZAjDGmChlCcAYY6JUWG0FISIN9N1AZjQygUMBDCfc2fvxV/ZefJa9H58VCe/HZFXNGlgYVglgLERk/WB7YUQrez/+yt6Lz7L347Mi+f2wLiBjjIlSlgCMMSZKRVMCeMLpAEKMvR9/Ze/FZ9n78VkR+35EzRiAMcaYz4qmFoAxxph+LAEYY0yUiooEICJLRWS7iOwSkR86HY9TRKRQRN4WkS0isllE7nE6plAgIm4RKReRV5yOxWkiki4iL4jINhHZKiKLnI7JKSJyr//vZJOIPC8i8U7HFGgRnwBExA38ArgMmAXcICKznI3KMT3A/1DVWcDZwLej+L3o7x5gq9NBhIhHgNdVtQSYR5S+LyKSD9wNLFTVOYAbuN7ZqAIv4hMAcCawS1X3qGoX8FvgGodjcoSq7lfVj/2PW+j74853NipniUgBcAXwS6djcZqIpAEXAE8BqGqXqnqdjcpRHiBBRDxAIlDncDwBFw0JIB+o7ve8hij/0AMQkSnAAuADZyNx3MPAfYDP6UBCQBHQADzj7xL7pYgkOR2UE1S1FvgpUAXsB5pU9U1nowq8aEgAZgARSQZeBL6jqs1Ox+MUEbkSqFfVDU7HEiI8wGnAY6q6AGgDonLMTEQm0NdTUATkAUki8nVnowq8aEgAtUBhv+cF/rKoJCIx9H34P6eqLzkdj8POBa4WkUr6ugaXiMivnQ3JUTVAjaoeaxW+QF9CiEYXA3tVtUFVu4GXgHMcjingoiEBfATMEJEiEYmlbyDnzw7H5AgREfr6d7eq6r87HY/TVPV+VS1Q1Sn0/V6sVNWI+5Y3Uqp6AKgWkWJ/URmwxcGQnFQFnC0iif6/mzIicEDc43QAwaaqPSJyJ/AGfSP5T6vqZofDcsq5wE3ApyJS4S/7n6r6qoMxmdByF/Cc/8vSHuBWh+NxhKp+ICIvAB/TN3uunAjcEsK2gjDGmCgVDV1AxhhjBmEJwBhjopQlAGOMiVKWAIwxJkpZAjDGmChlCcAYY6KUJQBjjIlS/x8E00Q+BVB77gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}